{
  "date": "2025-10-29",
  "title": "Daily Research Bulletin: artificial intelligence",
  "topics": [
    "artificial intelligence"
  ],
  "editorial": "Today's highlights in artificial intelligence: LLMs Struggle with VR Game Device Manipulation on New Benchmark (arXiv);  (arXiv); Agent Data Protocol Unifies Datasets, Boosts LLM Agent Performance (arXiv).",
  "sections": [
    {
      "id": "top_research",
      "title": "Top Research Picks",
      "items": [
        {
          "id": "8a2aff8eaf968080",
          "url": "http://arxiv.org/abs/2510.24706v1",
          "title": "ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?",
          "authors": [
            "Shuqing Li",
            "Jiayi Yan",
            "Chenyu Niu",
            "Jen-tse Huang",
            "Yun Peng",
            "Wenxuan Wang",
            "Yepang Liu",
            "Michael R. Lyu"
          ],
          "abstract": "Virtual Reality (VR) games require players to translate high-level semantic\nactions into precise device manipulations using controllers and head-mounted\ndisplays (HMDs). While humans intuitively perform this translation based on\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\ncan effectively replicate this ability remains underexplored. This paper\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\nsemantic actions into VR device manipulation sequences across 262 scenarios\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\nannotated ground truth and human performance. Our results reveal that while\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\ncapabilities, they still struggle with procedural reasoning and spatial\nunderstanding compared to humans. Performance varies significantly across\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\nsubstantially improve performance, indicating potential for targeted\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\nhttps://sites.google.com/view/combobench.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24706v1.pdf",
          "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.SE"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24706",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 47120,
            "final_url": "https://arxiv.org/abs/2510.24706v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24706v1] ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24706\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24706v1: ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24706v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"Virtual Reality (VR) games require players to translate high-level semantic actions into precise device manipulations using controllers and head-mounted displays (HMDs). While humans intuitively perform this translation based on common sense and embodied understanding, whether Large Language Models (LLMs) can effectively replicate this ability remains underexplored. This paper introduces a benchmark, ComboBench, evaluating LLMs' capability to translate semantic actions into VR device manipulation sequences across 262 scenarios from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II, and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o, Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against annotated ground truth and human performance. Our results reveal that while top-performing models like Gemini-1.5-Pro demonstrate strong task decomposition capabilities, they still struggle with procedural reasoning and spatial understanding compared to humans. Performance varies significantly across games, suggesting sensitivity to interaction complexity. Few-shot examples substantially improve performance, indicating potential for targeted enhancement of LLMs' VR manipulation capabilities. We release all materials at https://sites.google.com/view/combobench.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual...\" name=\"twitter:title\"/>\n<meta content=\"Virtual Reality (VR) games require players to translate high-level semantic actions into precise device manipulations using controllers and head-mounted displays (HMDs). While humans intuitively...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?\" name=\"citation_title\"/><meta content=\"Li, Shuqing\" name=\"citation_author\"/><meta content=\"Yan, Jiayi\" name=\"citation_author\"/><meta content=\"Niu, Chenyu\" name=\"citation_author\"/><meta content=\"Huang, Jen-tse\" name=\"citation_author\"/><meta content=\"Peng, Yun\" name=\"citation_author\"/><meta content=\"Wang, Wenxuan\" name=\"citation_author\"/><meta content=\"Liu, Yepang\" name=\"citation_author\"/><meta content=\"Lyu, Michael R.\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24706\" name=\"citation_pdf_url\"/><meta content=\"2510.24706\" name=\"citation_arxiv_id\"/><meta content=\"Virtual Reality (VR) games require players to translate high-level semantic actions into precise device manipulations using controllers and head-mounted displays (HMDs). While humans intuitively perform this translation based on common sense and embodied understanding, whether Large Language Models (LLMs) can effectively replicate this ability remains underexplored. This paper introduces a benchmark, ComboBench, evaluating LLMs' capability to translate semantic actions into VR device manipulation sequences across 262 scenarios from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II, and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o, Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against annotated ground truth and human performance. Our results reveal that while top-performing models like Gemini-1.5-Pro demonstrate strong task decomposition capabilities, they still struggle with procedural reasoning and spatial understanding compared to humans. Performance varies significantly across games, suggesting sensitivity to interaction complexity. Few-shot examples substantially improve performance, indicating potential for targeted enhancement of LLMs' VR manipulation capabilities. We release all materials at https://sites.google.com/view/combobench.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24706v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option value=\"orcid\">ORCID</option>\n<option value=\"author_id\">arXiv author ID</option>\n<option value=\"help\">Help pages</option>\n<option value=\"full_text\">Full text</option>\n</select>\n</div>\n</div>\n<input name=\"source\" type=\"hidden\" value=\"header\"/>\n<button class=\"button is-small is-cul-darker\">Search</button>\n</div>\n</form>\n</div>\n</div><!-- /end desktop header -->\n<div class=\"mobile-header\">\n<div class=\"columns is-mobile\">\n<div class=\"column logo-arxiv\"><a href=\"https://arxiv.org/\"><img alt=\"arXiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logomark-s",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "LLMs Struggle with VR Game Device Manipulation on New Benchmark",
          "tldr": "ComboBench, a new benchmark, evaluates Large Language Models' (LLMs) ability to translate high-level semantic actions into precise VR device manipulations across 262 scenarios from four popular VR games. While top LLMs like Gemini-1.5-Pro demonstrate strong task decomposition, they significantly lag human performance in procedural reasoning and spatial understanding. Few-shot examples notably improve their capabilities.",
          "bullets": [
            "Introduces ComboBench, a benchmark evaluating LLMs' capability to translate semantic actions into VR device manipulation sequences.",
            "The benchmark comprises 262 scenarios from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II, and Vivecraft.",
            "Seven LLMs, including GPT-3.5, GPT-4o, and Gemini-1.5-Pro, were evaluated against human performance and ground truth.",
            "Top-performing models like Gemini-1.5-Pro exhibit strong task decomposition but struggle with procedural reasoning and spatial understanding compared to humans.",
            "LLM performance varies significantly across different games, indicating sensitivity to interaction complexity.",
            "Few-shot examples substantially improve LLMs' performance in VR manipulation tasks.",
            "All benchmark materials are publicly released to facilitate further research."
          ],
          "significance": "This research is crucial for understanding LLMs' potential in embodied AI and human-computer interaction, particularly in complex virtual environments. Developing LLMs capable of precise VR manipulation could lead to more intuitive AI agents and advanced assistive technologies in virtual spaces.",
          "limitations": "Current LLMs, even top performers, struggle with the procedural reasoning and spatial understanding required for complex VR device manipulations, falling short of human capabilities.",
          "keywords": [
            "Large Language Models",
            "Virtual Reality",
            "Game AI",
            "Benchmarking",
            "Embodied AI",
            "Human-Computer Interaction"
          ],
          "read_time_minutes": 20
        },
        {
          "id": "d06bdfcd43053f41",
          "url": "http://arxiv.org/abs/2510.24709v1",
          "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?",
          "authors": [
            "Yihao Li",
            "Saeed Salehi",
            "Lyle Ungar",
            "Konrad P. Kording"
          ],
          "abstract": "Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24709v1.pdf",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "q-bio.NC"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24709",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 48207,
            "final_url": "https://arxiv.org/abs/2510.24709v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24709v1] Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24709\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24709v1: Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24709v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of &quot;which parts belong together&quot; emerges naturally in a connectionist system.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"Does Object Binding Naturally Emerge in Large Pretrained Vision...\" name=\"twitter:title\"/>\n<meta content=\"Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?\" name=\"citation_title\"/><meta content=\"Li, Yihao\" name=\"citation_author\"/><meta content=\"Salehi, Saeed\" name=\"citation_author\"/><meta content=\"Ungar, Lyle\" name=\"citation_author\"/><meta content=\"Kording, Konrad P.\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24709\" name=\"citation_pdf_url\"/><meta content=\"2510.24709\" name=\"citation_arxiv_id\"/><meta content=\"Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of &quot;which parts belong together&quot; emerges naturally in a connectionist system.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24709v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</o",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": ""
        },
        {
          "id": "2cd9c52c324c81e9",
          "url": "http://arxiv.org/abs/2510.24702v1",
          "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents",
          "authors": [
            "Yueqi Song",
            "Ketan Ramaneti",
            "Zaid Sheikh",
            "Ziru Chen",
            "Boyu Gou",
            "Tianbao Xie",
            "Yiheng Xu",
            "Danyang Zhang",
            "Apurva Gandhi",
            "Fan Yang",
            "Joseph Liu",
            "Tianyue Ou",
            "Zhihao Yuan",
            "Frank Xu",
            "Shuyan Zhou",
            "Xingyao Wang",
            "Xiang Yue",
            "Tao Yu",
            "Huan Sun",
            "Yu Su",
            "Graham Neubig"
          ],
          "abstract": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24702v1.pdf",
          "categories": [
            "cs.CL",
            "cs.AI"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24702",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 49222,
            "final_url": "https://arxiv.org/abs/2510.24702v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24702v1] Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24702\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24702v1: Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24702v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content='Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an \"interlingua\" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.' property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"Agent Data Protocol: Unifying Datasets for Diverse, Effective...\" name=\"twitter:title\"/>\n<meta content=\"Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents\" name=\"citation_title\"/><meta content=\"Song, Yueqi\" name=\"citation_author\"/><meta content=\"Ramaneti, Ketan\" name=\"citation_author\"/><meta content=\"Sheikh, Zaid\" name=\"citation_author\"/><meta content=\"Chen, Ziru\" name=\"citation_author\"/><meta content=\"Gou, Boyu\" name=\"citation_author\"/><meta content=\"Xie, Tianbao\" name=\"citation_author\"/><meta content=\"Xu, Yiheng\" name=\"citation_author\"/><meta content=\"Zhang, Danyang\" name=\"citation_author\"/><meta content=\"Gandhi, Apurva\" name=\"citation_author\"/><meta content=\"Yang, Fan\" name=\"citation_author\"/><meta content=\"Liu, Joseph\" name=\"citation_author\"/><meta content=\"Ou, Tianyue\" name=\"citation_author\"/><meta content=\"Yuan, Zhihao\" name=\"citation_author\"/><meta content=\"Xu, Frank\" name=\"citation_author\"/><meta content=\"Zhou, Shuyan\" name=\"citation_author\"/><meta content=\"Wang, Xingyao\" name=\"citation_author\"/><meta content=\"Yue, Xiang\" name=\"citation_author\"/><meta content=\"Yu, Tao\" name=\"citation_author\"/><meta content=\"Sun, Huan\" name=\"citation_author\"/><meta content=\"Su, Yu\" name=\"citation_author\"/><meta content=\"Neubig, Graham\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24702\" name=\"citation_pdf_url\"/><meta content=\"2510.24702\" name=\"citation_arxiv_id\"/><meta content='Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an \"interlingua\" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.' name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24702v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Autho",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "Agent Data Protocol Unifies Datasets, Boosts LLM Agent Performance",
          "tldr": "Researchers introduce the Agent Data Protocol (ADP), a lightweight representation language designed to unify fragmented agent training data. ADP acts as an \"interlingua\" for diverse datasets, simplifying downstream training pipelines. Experiments unifying 13 existing datasets demonstrated an average ~20% performance gain over base models and achieved state-of-the-art results on various agent benchmarks.",
          "bullets": [
            "The challenge of collecting agent training data is identified as fragmentation across heterogeneous formats, tools, and interfaces.",
            "The Agent Data Protocol (ADP) is introduced as a lightweight \"interlingua\" to unify diverse agent datasets for downstream training.",
            "ADP is designed to be expressive, capturing tasks like API/tool use, browsing, coding, and general agentic workflows, while remaining simple to parse.",
            "Researchers unified 13 existing agent training datasets into the ADP format and converted them for multiple agent frameworks.",
            "Supervised fine-tuning (SFT) on ADP-formatted data resulted in an average performance gain of ~20% over corresponding base models.",
            "The fine-tuned models achieved state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks without domain-specific tuning.",
            "All code and data for ADP are publicly released to facilitate standardized, scalable, and reproducible agent training."
          ],
          "significance": "This work addresses a critical bottleneck in large-scale supervised fine-tuning of AI agents by providing a standardized data representation. By unifying fragmented datasets, ADP lowers the barrier for researchers and developers to train more effective LLM agents. The demonstrated performance improvements across diverse tasks highlight the potential for more robust and generalizable agent capabilities.",
          "limitations": "The abstract does not explicitly state limitations of the Agent Data Protocol. However, future work could focus on ensuring its extensibility to an even broader and more complex array of agentic tasks, beyond the 'large variety' currently captured.",
          "keywords": [
            "Agent training",
            "LLM agents",
            "data protocol",
            "supervised fine-tuning",
            "unified datasets",
            "API/tool use",
            "coding"
          ],
          "read_time_minutes": 15
        }
      ]
    }
  ],
  "metadata": {
    "generated_at": "2025-10-29T16:54:50.632018",
    "sources_count": 1,
    "papers_count": 3,
    "news_count": 0,
    "total_items": 3
  }
}