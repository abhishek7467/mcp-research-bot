{
  "date": "2025-10-29",
  "title": "Daily Research Bulletin: artificial intelligence",
  "topics": [
    "artificial intelligence"
  ],
  "editorial": "Today's highlights in artificial intelligence: Tongyi DeepResearch: Agentic LLM Excels in Deep Information-Seeking Tasks (arXiv); Greedy Sampling Proves Efficient for RLHF Theoretical Guarantees (arXiv);  (arXiv).",
  "sections": [
    {
      "id": "top_research",
      "title": "Top Research Picks",
      "items": [
        {
          "id": "ed54eda8993dc5bf",
          "url": "http://arxiv.org/abs/2510.24701v1",
          "title": "Tongyi DeepResearch Technical Report",
          "authors": [
            "Tongyi DeepResearch Team",
            "Baixuan Li",
            "Bo Zhang",
            "Dingchu Zhang",
            "Fei Huang",
            "Guangyu Li",
            "Guoxin Chen",
            "Huifeng Yin",
            "Jialong Wu",
            "Jingren Zhou",
            "Kuan Li",
            "Liangcai Su",
            "Litu Ou",
            "Liwen Zhang",
            "Pengjun Xie",
            "Rui Ye",
            "Wenbiao Yin",
            "Xinmiao Yu",
            "Xinyu Wang",
            "Xixi Wu",
            "Xuanzhong Chen",
            "Yida Zhao",
            "Zhen Zhang",
            "Zhengwei Tao",
            "Zhongwang Zhang",
            "Zile Qiao",
            "Chenxi Wang",
            "Donglei Yu",
            "Gang Fu",
            "Haiyang Shen",
            "Jiayin Yang",
            "Jun Lin",
            "Junkai Zhang",
            "Kui Zeng",
            "Li Yang",
            "Hailong Yin",
            "Maojia Song",
            "Ming Yan",
            "Peng Xia",
            "Qian Xiao",
            "Rui Min",
            "Ruixue Ding",
            "Runnan Fang",
            "Shaowei Chen",
            "Shen Huang",
            "Shihang Wang",
            "Shihao Cai",
            "Weizhou Shen",
            "Xiaobin Wang",
            "Xin Guan",
            "Xinyu Geng",
            "Yingcheng Shi",
            "Yuning Wu",
            "Zhuo Chen",
            "Zijian Li",
            "Yong Jiang"
          ],
          "abstract": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24701v1.pdf",
          "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG",
            "cs.MA"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24701",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 53915,
            "final_url": "https://arxiv.org/abs/2510.24701v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24701v1] Tongyi DeepResearch Technical Report</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24701\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24701v1: Tongyi DeepResearch Technical Report\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"Tongyi DeepResearch Technical Report\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24701v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"Tongyi DeepResearch Technical Report\" name=\"twitter:title\"/>\n<meta content=\"We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"Tongyi DeepResearch Technical Report\" name=\"citation_title\"/><meta content=\"Tongyi DeepResearch Team\" name=\"citation_author\"/><meta content=\"Li, Baixuan\" name=\"citation_author\"/><meta content=\"Zhang, Bo\" name=\"citation_author\"/><meta content=\"Zhang, Dingchu\" name=\"citation_author\"/><meta content=\"Huang, Fei\" name=\"citation_author\"/><meta content=\"Li, Guangyu\" name=\"citation_author\"/><meta content=\"Chen, Guoxin\" name=\"citation_author\"/><meta content=\"Yin, Huifeng\" name=\"citation_author\"/><meta content=\"Wu, Jialong\" name=\"citation_author\"/><meta content=\"Zhou, Jingren\" name=\"citation_author\"/><meta content=\"Li, Kuan\" name=\"citation_author\"/><meta content=\"Su, Liangcai\" name=\"citation_author\"/><meta content=\"Ou, Litu\" name=\"citation_author\"/><meta content=\"Zhang, Liwen\" name=\"citation_author\"/><meta content=\"Xie, Pengjun\" name=\"citation_author\"/><meta content=\"Ye, Rui\" name=\"citation_author\"/><meta content=\"Yin, Wenbiao\" name=\"citation_author\"/><meta content=\"Yu, Xinmiao\" name=\"citation_author\"/><meta content=\"Wang, Xinyu\" name=\"citation_author\"/><meta content=\"Wu, Xixi\" name=\"citation_author\"/><meta content=\"Chen, Xuanzhong\" name=\"citation_author\"/><meta content=\"Zhao, Yida\" name=\"citation_author\"/><meta content=\"Zhang, Zhen\" name=\"citation_author\"/><meta content=\"Tao, Zhengwei\" name=\"citation_author\"/><meta content=\"Zhang, Zhongwang\" name=\"citation_author\"/><meta content=\"Qiao, Zile\" name=\"citation_author\"/><meta content=\"Wang, Chenxi\" name=\"citation_author\"/><meta content=\"Yu, Donglei\" name=\"citation_author\"/><meta content=\"Fu, Gang\" name=\"citation_author\"/><meta content=\"Shen, Haiyang\" name=\"citation_author\"/><meta content=\"Yang, Jiayin\" name=\"citation_author\"/><meta content=\"Lin, Jun\" name=\"citation_author\"/><meta content=\"Zhang, Junkai\" name=\"citation_author\"/><meta content=\"Zeng, Kui\" name=\"citation_author\"/><meta content=\"Yang, Li\" name=\"citation_author\"/><meta content=\"Yin, Hailong\" name=\"citation_author\"/><meta content=\"Song, Maojia\" name=\"citation_author\"/><meta content=\"Yan, Ming\" name=\"citation_author\"/><meta content=\"Xia, Peng\" name=\"citation_author\"/><meta content=\"Xiao, Qian\" name=\"citation_author\"/><meta content=\"Min, Rui\" name=\"citation_author\"/><meta content=\"Ding, Ruixue\" name=\"citation_author\"/><meta content=\"Fang, Runnan\" name=\"citation_author\"/><meta content=\"Chen, Shaowei\" name=\"citation_author\"/><meta content=\"Huang, Shen\" name=\"citation_author\"/><meta content=\"Wang, Shihang\" name=\"citation_author\"/><meta content=\"Cai, Shihao\" name=\"citation_author\"/><meta content=\"Shen, Weizhou\" name=\"citation_author\"/><meta content=\"Wang, Xiaobin\" name=\"citation_author\"/><meta content=\"Guan, Xin\" name=\"citation_author\"/><meta content=\"Geng, Xinyu\" name=\"citation_author\"/><meta content=\"Shi, Yingcheng\" name=\"citation_author\"/><meta content=\"Wu, Yuning\" name=\"citation_author\"/><meta content=\"Chen, Zhuo\" name=\"citation_author\"/><meta content=\"Li, Zijian\" name=\"citation_author\"/><meta content=\"Jiang, Yong\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24701\" name=\"citation_pdf_url\"/><meta content=\"2510.24701\" name=\"citation_arxiv_id\"/><meta content=\"We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-colo",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "Tongyi DeepResearch: Agentic LLM Excels in Deep Information-Seeking Tasks",
          "tldr": "Tongyi DeepResearch is an agentic large language model specifically designed for long-horizon, deep information-seeking research. It utilizes an end-to-end training framework, combining agentic mid-training and post-training with a scalable, automatic data synthesis pipeline. The 30.5 billion parameter model achieves state-of-the-art performance across multiple agentic deep research benchmarks.",
          "bullets": [
            "Tongyi DeepResearch is an agentic large language model tailored for long-horizon, deep information-seeking research tasks.",
            "It employs an end-to-end training framework integrating agentic mid-training and post-training for scalable reasoning.",
            "A fully automatic, human-annotation-free data synthesis pipeline supports all training stages.",
            "Customized environments ensure stable and consistent interactions throughout the system's development.",
            "The model features 30.5 billion total parameters, with only 3.3 billion activated per token.",
            "It achieves state-of-the-art performance on benchmarks including Humanity's Last Exam, BrowseComp, and xbench-DeepSearch.",
            "The model, framework, and complete solutions are open-sourced to benefit the research community."
          ],
          "significance": "This development offers a significant step towards automating complex, deep information-seeking research, potentially accelerating scientific discovery and knowledge synthesis. By providing a scalable and autonomous agentic LLM, Tongyi DeepResearch could transform how researchers approach extensive data exploration and problem-solving.",
          "limitations": "The abstract does not detail specific limitations or potential challenges encountered during development or deployment.",
          "keywords": [
            "Agentic LLM",
            "Deep Research",
            "Information Seeking",
            "Large Language Model",
            "End-to-End Training",
            "State-of-the-Art",
            "Open-Source"
          ],
          "read_time_minutes": 15
        },
        {
          "id": "6bbed0ed54ed3ea7",
          "url": "http://arxiv.org/abs/2510.24700v1",
          "title": "Greedy Sampling Is Provably Efficient for RLHF",
          "authors": [
            "Di Wu",
            "Chengshuai Shi",
            "Jing Yang",
            "Cong Shen"
          ],
          "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a key\ntechnique for post-training large language models. Despite its empirical\nsuccess, the theoretical understanding of RLHF is still limited, as learning\nthe KL-regularized target with only preference feedback poses additional\nchallenges compared with canonical RL. Existing works mostly study the\nreward-based Bradley-Terry (BT) preference model, and extend classical designs\nutilizing optimism or pessimism. This work, instead, considers the general\npreference model (whose practical relevance has been observed recently) and\nobtains performance guarantees with major, order-wise improvements over\nexisting ones. Surprisingly, these results are derived from algorithms that\ndirectly use the empirical estimates (i.e., greedy sampling), as opposed to\nconstructing optimistic or pessimistic estimates in previous works. This\ninsight has a deep root in the unique structural property of the optimal policy\nclass under the KL-regularized target, and we further specialize it to the BT\nmodel, highlighting the surprising sufficiency of greedy sampling in RLHF.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24700v1.pdf",
          "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT",
            "stat.ML"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24700",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 46580,
            "final_url": "https://arxiv.org/abs/2510.24700v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24700v1] Greedy Sampling Is Provably Efficient for RLHF</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24700\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24700v1: Greedy Sampling Is Provably Efficient for RLHF\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"Greedy Sampling Is Provably Efficient for RLHF\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24700v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique for post-training large language models. Despite its empirical success, the theoretical understanding of RLHF is still limited, as learning the KL-regularized target with only preference feedback poses additional challenges compared with canonical RL. Existing works mostly study the reward-based Bradley-Terry (BT) preference model, and extend classical designs utilizing optimism or pessimism. This work, instead, considers the general preference model (whose practical relevance has been observed recently) and obtains performance guarantees with major, order-wise improvements over existing ones. Surprisingly, these results are derived from algorithms that directly use the empirical estimates (i.e., greedy sampling), as opposed to constructing optimistic or pessimistic estimates in previous works. This insight has a deep root in the unique structural property of the optimal policy class under the KL-regularized target, and we further specialize it to the BT model, highlighting the surprising sufficiency of greedy sampling in RLHF.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"Greedy Sampling Is Provably Efficient for RLHF\" name=\"twitter:title\"/>\n<meta content=\"Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique for post-training large language models. Despite its empirical success, the theoretical understanding of RLHF is...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"Greedy Sampling Is Provably Efficient for RLHF\" name=\"citation_title\"/><meta content=\"Wu, Di\" name=\"citation_author\"/><meta content=\"Shi, Chengshuai\" name=\"citation_author\"/><meta content=\"Yang, Jing\" name=\"citation_author\"/><meta content=\"Shen, Cong\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24700\" name=\"citation_pdf_url\"/><meta content=\"2510.24700\" name=\"citation_arxiv_id\"/><meta content=\"Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique for post-training large language models. Despite its empirical success, the theoretical understanding of RLHF is still limited, as learning the KL-regularized target with only preference feedback poses additional challenges compared with canonical RL. Existing works mostly study the reward-based Bradley-Terry (BT) preference model, and extend classical designs utilizing optimism or pessimism. This work, instead, considers the general preference model (whose practical relevance has been observed recently) and obtains performance guarantees with major, order-wise improvements over existing ones. Surprisingly, these results are derived from algorithms that directly use the empirical estimates (i.e., greedy sampling), as opposed to constructing optimistic or pessimistic estimates in previous works. This insight has a deep root in the unique structural property of the optimal policy class under the KL-regularized target, and we further specialize it to the BT model, highlighting the surprising sufficiency of greedy sampling in RLHF.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24700v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option value=\"orcid\">ORCID</option>\n<option value=\"author_id\">arXiv author ID</option>\n<option value=\"help\">Help pages</option>\n<option value=\"full_text\">Full text</option>\n</select>\n</div>\n</div>\n<input name=\"source\" type=\"hidden\" value=\"header\"/>\n<button class=\"button is-small is-cul-darker\">Search</button>\n</div>\n</form>\n</div>\n</div><!-- /end desktop header -->\n<div class=\"mobile-header\">\n<div class=\"columns is-mobile\">\n<div class=\"column logo-arxiv\"><a href=\"https://arxiv.org/\"><img alt=\"arXiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logomark-small-white.svg\" style=\"height:60px;\"/></a></div>\n<div class=\"column logo-cornell\"><a href=\"https://www.cornell.edu/\">\n<picture>\n<source media=\"(min-width: 501px)\" sizes=\"400w\" srcset=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w\"/>\n<source srcset=\"/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x\"/>\n<img alt=\"Cornell University Logo\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/>\n</picture>\n</a></div>\n<div class=\"column nav\" id=\"toggle-container\" role=\"menubar\">\n<button class=\"toggle-control\"><svg class=\"icon filter-white\" viewbox=\"0 0 512 512\" xmlns=\"http://www.w3.org/2000/svg\"><title>open search</title><path d=\"M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C4",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "Greedy Sampling Proves Efficient for RLHF Theoretical Guarantees",
          "tldr": "This research addresses the limited theoretical understanding of Reinforcement Learning from Human Feedback (RLHF), particularly for KL-regularized targets with preference feedback. The authors introduce a novel approach using greedy sampling with a general preference model, achieving significant performance improvements. This work demonstrates that simple greedy sampling is surprisingly sufficient for obtaining strong theoretical guarantees in RLHF.",
          "bullets": [
            "The study advances the theoretical understanding of RLHF, which is crucial for post-training large language models.",
            "It considers a general preference model, moving beyond the commonly studied reward-based Bradley-Terry (BT) model.",
            "The work achieves major, order-wise improvements in performance guarantees compared to existing methods.",
            "These improved results are derived from algorithms utilizing direct empirical estimates (greedy sampling), rather than complex optimistic or pessimistic constructions.",
            "The findings are rooted in a unique structural property of the optimal policy class under the KL-regularized target.",
            "The research further specializes its insights to the BT model, confirming the surprising sufficiency of greedy sampling in this context."
          ],
          "significance": "This research significantly enhances the theoretical foundation of RLHF, a critical technique for developing advanced large language models. By demonstrating the provable efficiency of greedy sampling and providing stronger performance guarantees for general preference models, it could lead to simpler, more robust, and more reliable algorithms for aligning AI with human preferences. This could accelerate the development of more effective and trustworthy AI systems.",
          "limitations": "The abstract does not explicitly detail specific limitations of the proposed theoretical framework or its practical implementation challenges.",
          "keywords": [
            "Reinforcement Learning from Human Feedback",
            "RLHF",
            "Greedy Sampling",
            "Preference Learning",
            "Theoretical Guarantees",
            "Large Language Models",
            "General Preference Model"
          ],
          "read_time_minutes": 20
        },
        {
          "id": "2cd9c52c324c81e9",
          "url": "http://arxiv.org/abs/2510.24702v1",
          "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents",
          "authors": [
            "Yueqi Song",
            "Ketan Ramaneti",
            "Zaid Sheikh",
            "Ziru Chen",
            "Boyu Gou",
            "Tianbao Xie",
            "Yiheng Xu",
            "Danyang Zhang",
            "Apurva Gandhi",
            "Fan Yang",
            "Joseph Liu",
            "Tianyue Ou",
            "Zhihao Yuan",
            "Frank Xu",
            "Shuyan Zhou",
            "Xingyao Wang",
            "Xiang Yue",
            "Tao Yu",
            "Huan Sun",
            "Yu Su",
            "Graham Neubig"
          ],
          "abstract": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24702v1.pdf",
          "categories": [
            "cs.CL",
            "cs.AI"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24702",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 49222,
            "final_url": "https://arxiv.org/abs/2510.24702v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24702v1] Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24702\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24702v1: Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24702v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content='Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an \"interlingua\" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.' property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"Agent Data Protocol: Unifying Datasets for Diverse, Effective...\" name=\"twitter:title\"/>\n<meta content=\"Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents\" name=\"citation_title\"/><meta content=\"Song, Yueqi\" name=\"citation_author\"/><meta content=\"Ramaneti, Ketan\" name=\"citation_author\"/><meta content=\"Sheikh, Zaid\" name=\"citation_author\"/><meta content=\"Chen, Ziru\" name=\"citation_author\"/><meta content=\"Gou, Boyu\" name=\"citation_author\"/><meta content=\"Xie, Tianbao\" name=\"citation_author\"/><meta content=\"Xu, Yiheng\" name=\"citation_author\"/><meta content=\"Zhang, Danyang\" name=\"citation_author\"/><meta content=\"Gandhi, Apurva\" name=\"citation_author\"/><meta content=\"Yang, Fan\" name=\"citation_author\"/><meta content=\"Liu, Joseph\" name=\"citation_author\"/><meta content=\"Ou, Tianyue\" name=\"citation_author\"/><meta content=\"Yuan, Zhihao\" name=\"citation_author\"/><meta content=\"Xu, Frank\" name=\"citation_author\"/><meta content=\"Zhou, Shuyan\" name=\"citation_author\"/><meta content=\"Wang, Xingyao\" name=\"citation_author\"/><meta content=\"Yue, Xiang\" name=\"citation_author\"/><meta content=\"Yu, Tao\" name=\"citation_author\"/><meta content=\"Sun, Huan\" name=\"citation_author\"/><meta content=\"Su, Yu\" name=\"citation_author\"/><meta content=\"Neubig, Graham\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24702\" name=\"citation_pdf_url\"/><meta content=\"2510.24702\" name=\"citation_arxiv_id\"/><meta content='Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an \"interlingua\" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.' name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24702v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Autho",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": ""
        },
        {
          "id": "d06bdfcd43053f41",
          "url": "http://arxiv.org/abs/2510.24709v1",
          "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?",
          "authors": [
            "Yihao Li",
            "Saeed Salehi",
            "Lyle Ungar",
            "Konrad P. Kording"
          ],
          "abstract": "Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24709v1.pdf",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "q-bio.NC"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24709",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 48207,
            "final_url": "https://arxiv.org/abs/2510.24709v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24709v1] Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24709\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24709v1: Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24709v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of &quot;which parts belong together&quot; emerges naturally in a connectionist system.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"Does Object Binding Naturally Emerge in Large Pretrained Vision...\" name=\"twitter:title\"/>\n<meta content=\"Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?\" name=\"citation_title\"/><meta content=\"Li, Yihao\" name=\"citation_author\"/><meta content=\"Salehi, Saeed\" name=\"citation_author\"/><meta content=\"Ungar, Lyle\" name=\"citation_author\"/><meta content=\"Kording, Konrad P.\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24709\" name=\"citation_pdf_url\"/><meta content=\"2510.24709\" name=\"citation_arxiv_id\"/><meta content=\"Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of &quot;which parts belong together&quot; emerges naturally in a connectionist system.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24709v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</o",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "Self-Supervised Vision Transformers Naturally Develop Object Binding",
          "tldr": "This research investigates whether object binding, the brain's ability to group features into coherent objects, naturally emerges in pretrained Vision Transformers (ViTs). The study hypothesizes ViTs represent an 'IsSameObject' property and decodes it from patch embeddings. Findings show this capability reliably emerges in self-supervised ViTs, challenging prior assumptions about their lack of object-centric understanding.",
          "bullets": [
            "Researchers hypothesized that Vision Transformers represent whether two patches belong to the same object, termed 'IsSameObject'.",
            "A similarity probe was used to decode 'IsSameObject' from patch embeddings across ViT layers, achieving over 90% accuracy.",
            "Object-binding capability emerged reliably in self-supervised ViTs (DINO, MAE, CLIP) but was markedly weaker in ImageNet-supervised models.",
            "This suggests object binding is acquired through specific pretraining objectives, not merely an architectural artifact.",
            "The 'IsSameObject' signal is encoded in a low-dimensional subspace on top of object features and actively guides attention.",
            "Ablating 'IsSameObject' from model activations degraded downstream performance, indicating its functional importance.",
            "The findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge emerges in connectionist systems."
          ],
          "significance": "This work challenges the prevailing view that Vision Transformers lack object binding, demonstrating an emergent symbolic understanding in connectionist systems. It suggests that specific self-supervised pretraining objectives are crucial for developing this fundamental cognitive ability, potentially leading to more human-like AI with improved reasoning and memory capabilities.",
          "limitations": "While demonstrating emergence, the study could further explore the precise learning mechanisms during pretraining and the generalizability of these findings across an even wider range of ViT architectures and downstream tasks.",
          "keywords": [
            "Vision Transformers",
            "Object Binding",
            "Self-Supervised Learning",
            "IsSameObject",
            "Attention Mechanisms",
            "Emergent Properties",
            "Cognitive AI"
          ],
          "read_time_minutes": 18
        },
        {
          "id": "8a2aff8eaf968080",
          "url": "http://arxiv.org/abs/2510.24706v1",
          "title": "ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?",
          "authors": [
            "Shuqing Li",
            "Jiayi Yan",
            "Chenyu Niu",
            "Jen-tse Huang",
            "Yun Peng",
            "Wenxuan Wang",
            "Yepang Liu",
            "Michael R. Lyu"
          ],
          "abstract": "Virtual Reality (VR) games require players to translate high-level semantic\nactions into precise device manipulations using controllers and head-mounted\ndisplays (HMDs). While humans intuitively perform this translation based on\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\ncan effectively replicate this ability remains underexplored. This paper\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\nsemantic actions into VR device manipulation sequences across 262 scenarios\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\nannotated ground truth and human performance. Our results reveal that while\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\ncapabilities, they still struggle with procedural reasoning and spatial\nunderstanding compared to humans. Performance varies significantly across\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\nsubstantially improve performance, indicating potential for targeted\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\nhttps://sites.google.com/view/combobench.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24706v1.pdf",
          "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.SE"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24706",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 47120,
            "final_url": "https://arxiv.org/abs/2510.24706v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24706v1] ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24706\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24706v1: ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24706v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"Virtual Reality (VR) games require players to translate high-level semantic actions into precise device manipulations using controllers and head-mounted displays (HMDs). While humans intuitively perform this translation based on common sense and embodied understanding, whether Large Language Models (LLMs) can effectively replicate this ability remains underexplored. This paper introduces a benchmark, ComboBench, evaluating LLMs' capability to translate semantic actions into VR device manipulation sequences across 262 scenarios from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II, and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o, Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against annotated ground truth and human performance. Our results reveal that while top-performing models like Gemini-1.5-Pro demonstrate strong task decomposition capabilities, they still struggle with procedural reasoning and spatial understanding compared to humans. Performance varies significantly across games, suggesting sensitivity to interaction complexity. Few-shot examples substantially improve performance, indicating potential for targeted enhancement of LLMs' VR manipulation capabilities. We release all materials at https://sites.google.com/view/combobench.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual...\" name=\"twitter:title\"/>\n<meta content=\"Virtual Reality (VR) games require players to translate high-level semantic actions into precise device manipulations using controllers and head-mounted displays (HMDs). While humans intuitively...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?\" name=\"citation_title\"/><meta content=\"Li, Shuqing\" name=\"citation_author\"/><meta content=\"Yan, Jiayi\" name=\"citation_author\"/><meta content=\"Niu, Chenyu\" name=\"citation_author\"/><meta content=\"Huang, Jen-tse\" name=\"citation_author\"/><meta content=\"Peng, Yun\" name=\"citation_author\"/><meta content=\"Wang, Wenxuan\" name=\"citation_author\"/><meta content=\"Liu, Yepang\" name=\"citation_author\"/><meta content=\"Lyu, Michael R.\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24706\" name=\"citation_pdf_url\"/><meta content=\"2510.24706\" name=\"citation_arxiv_id\"/><meta content=\"Virtual Reality (VR) games require players to translate high-level semantic actions into precise device manipulations using controllers and head-mounted displays (HMDs). While humans intuitively perform this translation based on common sense and embodied understanding, whether Large Language Models (LLMs) can effectively replicate this ability remains underexplored. This paper introduces a benchmark, ComboBench, evaluating LLMs' capability to translate semantic actions into VR device manipulation sequences across 262 scenarios from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II, and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o, Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against annotated ground truth and human performance. Our results reveal that while top-performing models like Gemini-1.5-Pro demonstrate strong task decomposition capabilities, they still struggle with procedural reasoning and spatial understanding compared to humans. Performance varies significantly across games, suggesting sensitivity to interaction complexity. Few-shot examples substantially improve performance, indicating potential for targeted enhancement of LLMs' VR manipulation capabilities. We release all materials at https://sites.google.com/view/combobench.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24706v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option value=\"orcid\">ORCID</option>\n<option value=\"author_id\">arXiv author ID</option>\n<option value=\"help\">Help pages</option>\n<option value=\"full_text\">Full text</option>\n</select>\n</div>\n</div>\n<input name=\"source\" type=\"hidden\" value=\"header\"/>\n<button class=\"button is-small is-cul-darker\">Search</button>\n</div>\n</form>\n</div>\n</div><!-- /end desktop header -->\n<div class=\"mobile-header\">\n<div class=\"columns is-mobile\">\n<div class=\"column logo-arxiv\"><a href=\"https://arxiv.org/\"><img alt=\"arXiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logomark-s",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "LLMs Struggle with VR Device Manipulation Despite Task Decomposition",
          "tldr": "This paper introduces ComboBench, a new benchmark evaluating Large Language Models' (LLMs) ability to translate semantic actions into precise VR device manipulations across 262 scenarios from four popular VR games. While top LLMs like Gemini-1.5-Pro show strong task decomposition, they significantly struggle with procedural reasoning and spatial understanding compared to human performance. Few-shot examples substantially improve their capabilities.",
          "bullets": [
            "ComboBench is introduced as a new benchmark to evaluate LLMs' capability in translating high-level semantic actions into precise VR device manipulation sequences.",
            "The benchmark comprises 262 scenarios from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II, and Vivecraft.",
            "Seven prominent LLMs, including GPT-3.5, GPT-4, GPT-4o, Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, were evaluated against human performance and ground truth.",
            "Top-performing models like Gemini-1.5-Pro demonstrate strong task decomposition but fall short in procedural reasoning and spatial understanding compared to humans.",
            "LLM performance varies significantly across different games, indicating sensitivity to interaction complexity.",
            "Providing few-shot examples substantially improves LLMs' VR manipulation capabilities, suggesting potential for targeted enhancement.",
            "All benchmark materials, including scenarios and evaluation data, are publicly released for further research."
          ],
          "significance": "This research is crucial for understanding the current limitations and potential of LLMs in embodied AI and human-computer interaction. Evaluating LLMs in complex VR environments provides insights into their ability to bridge high-level commands with low-level physical actions, paving the way for more intuitive and capable AI agents in virtual and potentially real-world settings.",
          "limitations": "Current LLMs, even top-performing ones, struggle with the procedural reasoning and spatial understanding required for precise VR device manipulation, performing below human levels. Performance is also highly sensitive to the complexity of interactions within different VR games.",
          "keywords": [
            "Large Language Models (LLMs)",
            "Virtual Reality (VR)",
            "Embodied AI",
            "Benchmarking",
            "Device Manipulation",
            "Procedural Reasoning",
            "Spatial Understanding"
          ],
          "read_time_minutes": 15
        }
      ]
    },
    {
      "id": "methods_tools",
      "title": "Methods & Tools",
      "items": [
        {
          "id": "doi:10.1109/IROS47612.2022.9982198",
          "url": "http://arxiv.org/abs/2510.24683v1",
          "title": "A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers",
          "authors": [
            "Caleb Escobedo",
            "Nataliya Nechyporenko",
            "Shreyas Kadekodi",
            "Alessandro Roncone"
          ],
          "abstract": "Real-time control is an essential aspect of safe robot operation in the real\nworld with dynamic objects. We present a framework for the analysis of\nobject-aware controllers, methods for altering a robot's motion to anticipate\nand avoid possible collisions. This framework is focused on three design\nconsiderations: kinematics, motion profiles, and virtual constraints.\nAdditionally, the analysis in this work relies on verification of robot\nbehaviors using fundamental robot-obstacle experimental scenarios. To showcase\nthe effectiveness of our method we compare three representative object-aware\ncontrollers. The comparison uses metrics originating from the design\nconsiderations. From the analysis, we find that the design of object-aware\ncontrollers often lacks kinematic considerations, continuity of control points,\nand stability in movement profiles. We conclude that this framework can be used\nin the future to design, compare, and benchmark obstacle avoidance methods.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24683v1.pdf",
          "categories": [
            "cs.RO"
          ],
          "doi": "10.1109/IROS47612.2022.9982198",
          "canonical_url": "https://arxiv.org/abs/2510.24683",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 46844,
            "final_url": "https://arxiv.org/abs/2510.24683v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24683v1] A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24683\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24683v1: A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24683v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"Real-time control is an essential aspect of safe robot operation in the real world with dynamic objects. We present a framework for the analysis of object-aware controllers, methods for altering a robot's motion to anticipate and avoid possible collisions. This framework is focused on three design considerations: kinematics, motion profiles, and virtual constraints. Additionally, the analysis in this work relies on verification of robot behaviors using fundamental robot-obstacle experimental scenarios. To showcase the effectiveness of our method we compare three representative object-aware controllers. The comparison uses metrics originating from the design considerations. From the analysis, we find that the design of object-aware controllers often lacks kinematic considerations, continuity of control points, and stability in movement profiles. We conclude that this framework can be used in the future to design, compare, and benchmark obstacle avoidance methods.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"A Framework for the Systematic Evaluation of Obstacle Avoidance...\" name=\"twitter:title\"/>\n<meta content=\"Real-time control is an essential aspect of safe robot operation in the real world with dynamic objects. We present a framework for the analysis of object-aware controllers, methods for altering a...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers\" name=\"citation_title\"/><meta content=\"Escobedo, Caleb\" name=\"citation_author\"/><meta content=\"Nechyporenko, Nataliya\" name=\"citation_author\"/><meta content=\"Kadekodi, Shreyas\" name=\"citation_author\"/><meta content=\"Roncone, Alessandro\" name=\"citation_author\"/><meta content=\"10.1109/IROS47612.2022.9982198\" name=\"citation_doi\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24683\" name=\"citation_pdf_url\"/><meta content=\"2510.24683\" name=\"citation_arxiv_id\"/><meta content=\"Real-time control is an essential aspect of safe robot operation in the real world with dynamic objects. We present a framework for the analysis of object-aware controllers, methods for altering a robot's motion to anticipate and avoid possible collisions. This framework is focused on three design considerations: kinematics, motion profiles, and virtual constraints. Additionally, the analysis in this work relies on verification of robot behaviors using fundamental robot-obstacle experimental scenarios. To showcase the effectiveness of our method we compare three representative object-aware controllers. The comparison uses metrics originating from the design considerations. From the analysis, we find that the design of object-aware controllers often lacks kinematic considerations, continuity of control points, and stability in movement profiles. We conclude that this framework can be used in the future to design, compare, and benchmark obstacle avoidance methods.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24683v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option value=\"orcid\">ORCID</option>\n<option value=\"author_id\">arXiv author ID</option>\n<option value=\"help\">Help pages</option>\n<option value=\"full_text\">Full text</option>\n</select>\n</div>\n</div>\n<input name=\"source\" type=\"hidden\" value=\"header\"/>\n<button class=\"button is-small is-cul-darker\">Search</button>\n</div>\n</form>\n</div>\n</div><!-- /end desktop header -->\n<div class=\"mobile-header\">\n<div class=\"columns is-mobile\">\n<div class=\"column logo-arxiv\"><a href=\"https://arxiv.org/\"><img alt=\"arXiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logomark-small-white.svg\" style=\"height:60px;\"/></a></div>\n<div class=\"column logo-cornell\"><a href=\"https://www.cornell.edu/\">\n<picture>\n<source media=\"(min-width: 501px)\" sizes=\"400w\" srcset=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w\"/>\n<source srcset=\"/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x\"/>\n<img alt=\"Cornell University Logo\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/>\n</picture>\n</a></div>\n<div class=\"column nav\" id=\"toggle-container\" role=\"menubar\">\n<button class=\"toggle-control\"><svg class=\"icon filter-white\" viewbox=\"0 0 512 512\" xmlns=\"http://www.w3.org/2000/svg\"><title>open search</title><path d=\"M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": ""
        },
        {
          "id": "15572c34b81cdc5c",
          "url": "http://arxiv.org/abs/2510.24690v1",
          "title": "Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning",
          "authors": [
            "Shengjie Liu",
            "Li Dong",
            "Zhenyu Zhang"
          ],
          "abstract": "We present a framework for uncovering and exploiting dependencies among tools\nand documents to enhance exemplar artifact generation. Our method begins by\nconstructing a tool knowledge graph from tool schemas,including descriptions,\narguments, and output payloads, using a DeepResearch-inspired analysis. In\nparallel, we derive a complementary knowledge graph from internal documents and\nSOPs, which is then fused with the tool graph. To generate exemplar plans, we\nadopt a deep-sparse integration strategy that aligns structural tool\ndependencies with procedural knowledge. Experiments demonstrate that this\nunified framework effectively models tool interactions and improves plan\ngeneration, underscoring the benefits of linking tool graphs with domain\nknowledge graphs for tool-augmented reasoning and planning.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24690v1.pdf",
          "categories": [
            "cs.AI"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24690",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 44795,
            "final_url": "https://arxiv.org/abs/2510.24690v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24690v1] Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24690\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24690v1: Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24690v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"We present a framework for uncovering and exploiting dependencies among tools and documents to enhance exemplar artifact generation. Our method begins by constructing a tool knowledge graph from tool schemas,including descriptions, arguments, and output payloads, using a DeepResearch-inspired analysis. In parallel, we derive a complementary knowledge graph from internal documents and SOPs, which is then fused with the tool graph. To generate exemplar plans, we adopt a deep-sparse integration strategy that aligns structural tool dependencies with procedural knowledge. Experiments demonstrate that this unified framework effectively models tool interactions and improves plan generation, underscoring the benefits of linking tool graphs with domain knowledge graphs for tool-augmented reasoning and planning.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"Bridging Tool Dependencies and Domain Knowledge: A Graph-Based...\" name=\"twitter:title\"/>\n<meta content=\"We present a framework for uncovering and exploiting dependencies among tools and documents to enhance exemplar artifact generation. Our method begins by constructing a tool knowledge graph from...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning\" name=\"citation_title\"/><meta content=\"Liu, Shengjie\" name=\"citation_author\"/><meta content=\"Dong, Li\" name=\"citation_author\"/><meta content=\"Zhang, Zhenyu\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24690\" name=\"citation_pdf_url\"/><meta content=\"2510.24690\" name=\"citation_arxiv_id\"/><meta content=\"We present a framework for uncovering and exploiting dependencies among tools and documents to enhance exemplar artifact generation. Our method begins by constructing a tool knowledge graph from tool schemas,including descriptions, arguments, and output payloads, using a DeepResearch-inspired analysis. In parallel, we derive a complementary knowledge graph from internal documents and SOPs, which is then fused with the tool graph. To generate exemplar plans, we adopt a deep-sparse integration strategy that aligns structural tool dependencies with procedural knowledge. Experiments demonstrate that this unified framework effectively models tool interactions and improves plan generation, underscoring the benefits of linking tool graphs with domain knowledge graphs for tool-augmented reasoning and planning.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24690v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option value=\"orcid\">ORCID</option>\n<option value=\"author_id\">arXiv author ID</option>\n<option value=\"help\">Help pages</option>\n<option value=\"full_text\">Full text</option>\n</select>\n</div>\n</div>\n<input name=\"source\" type=\"hidden\" value=\"header\"/>\n<button class=\"button is-small is-cul-darker\">Search</button>\n</div>\n</form>\n</div>\n</div><!-- /end desktop header -->\n<div class=\"mobile-header\">\n<div class=\"columns is-mobile\">\n<div class=\"column logo-arxiv\"><a href=\"https://arxiv.org/\"><img alt=\"arXiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logomark-small-white.svg\" style=\"height:60px;\"/></a></div>\n<div class=\"column logo-cornell\"><a href=\"https://www.cornell.edu/\">\n<picture>\n<source media=\"(min-width: 501px)\" sizes=\"400w\" srcset=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w\"/>\n<source srcset=\"/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x\"/>\n<img alt=\"Cornell University Logo\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/>\n</picture>\n</a></div>\n<div class=\"column nav\" id=\"toggle-container\" role=\"menubar\">\n<button class=\"toggle-control\"><svg class=\"icon filter-white\" viewbox=\"0 0 512 512\" xmlns=\"http://www.w3.org/2000/svg\"><title>open search</title><path d=\"M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z\"></path></svg></button>\n<div class=\"mobile-toggle-block toggle-target\">\n<form action=\"https://arxiv.org/search\" class=\"mobile-search-form\" method=\"GET\">\n<div class=\"field has-a",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": ""
        }
      ]
    },
    {
      "id": "full_summaries",
      "title": "Full Summaries",
      "items": [
        {
          "id": "6166e7a686fb26da",
          "url": "http://arxiv.org/abs/2510.24698v1",
          "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking",
          "authors": [
            "Baixuan Li",
            "Dingchu Zhang",
            "Jialong Wu",
            "Wenbiao Yin",
            "Zhengwei Tao",
            "Yida Zhao",
            "Liwen Zhang",
            "Haiyang Shen",
            "Runnan Fang",
            "Pengjun Xie",
            "Jingren Zhou",
            "Yong Jiang"
          ],
          "abstract": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24698v1.pdf",
          "categories": [
            "cs.CL",
            "cs.AI"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24698",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 46657,
            "final_url": "https://arxiv.org/abs/2510.24698v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24698v1] ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24698\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24698v1: ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24698v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking\" name=\"twitter:title\"/>\n<meta content=\"Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking\" name=\"citation_title\"/><meta content=\"Li, Baixuan\" name=\"citation_author\"/><meta content=\"Zhang, Dingchu\" name=\"citation_author\"/><meta content=\"Wu, Jialong\" name=\"citation_author\"/><meta content=\"Yin, Wenbiao\" name=\"citation_author\"/><meta content=\"Tao, Zhengwei\" name=\"citation_author\"/><meta content=\"Zhao, Yida\" name=\"citation_author\"/><meta content=\"Zhang, Liwen\" name=\"citation_author\"/><meta content=\"Shen, Haiyang\" name=\"citation_author\"/><meta content=\"Fang, Runnan\" name=\"citation_author\"/><meta content=\"Xie, Pengjun\" name=\"citation_author\"/><meta content=\"Zhou, Jingren\" name=\"citation_author\"/><meta content=\"Jiang, Yong\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24698\" name=\"citation_pdf_url\"/><meta content=\"2510.24698\" name=\"citation_arxiv_id\"/><meta content=\"Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24698v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option value=\"orcid\">ORCID</option>\n<option value=\"author_id\">arXiv author ID</option>\n<option value=\"help\">Help pages</option>\n<option value=\"full_text\">Full text</option>\n</select>\n</div>\n</div>\n<input name=\"source\" type=\"hidden\" value=\"header\"/>\n<button class=\"button is-small is-cul-darker\">Search</button>\n</div>\n</form>\n</div>\n</div><!-- /end desktop header -->\n<div class=\"mobile-header\">\n<div class=\"columns is-mobile\">\n<div class=\"column logo-arxiv\"><a href=\"https://arxiv.org/\"><img alt=\"arXiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logomark-small-white.svg\" style=\"height:60px;\"/></a></div>\n<div class=\"column logo-cornell\"><a href=\"https://www.cornell.edu/\">\n<picture>\n<source media=\"(min-width: 501px)\" sizes=\"400w\" srcset=\"/static/browse/0.3.4/images/i",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "ParallelMuse Boosts Agentic Parallel Thinking for Deep Information Seeking",
          "tldr": "ParallelMuse is a novel two-stage paradigm designed to enhance deep information-seeking agents by addressing inefficiencies and context limitations in conventional parallel thinking. It improves exploration efficiency through path reuse and synthesizes coherent answers via lossless reasoning compression. The method achieves up to 62% performance improvement with a 10-30% reduction in exploratory token consumption.",
          "bullets": [
            "Conventional parallel thinking for information-seeking agents faces challenges in inefficiency and integrating long-horizon reasoning due to limited context.",
            "ParallelMuse proposes a two-stage paradigm to enhance exploration breadth and problem-solving capabilities for deep information-seeking agents.",
            "The first stage, Functionality-Specified Partial Rollout, partitions sequences into functional regions and uses uncertainty-guided path reuse and branching for efficient exploration.",
            "The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information vital for answer derivation.",
            "This approach synthesizes a coherent final answer by effectively integrating compressed reasoning trajectories.",
            "Experiments show ParallelMuse delivers up to a 62% performance improvement across multiple open-source agents and benchmarks.",
            "The method also reduces exploratory token consumption by 10-30%, indicating improved efficiency."
          ],
          "significance": "This research significantly advances the capabilities of information-seeking agents by making parallel thinking more efficient and effective. By overcoming key limitations in current methods, ParallelMuse enables agents to explore more broadly and integrate complex reasoning trajectories, leading to substantial improvements in problem-solving. This could impact various applications requiring deep information retrieval and complex decision-making.",
          "limitations": "The provided abstract does not explicitly detail specific limitations of the ParallelMuse method or areas for future work.",
          "keywords": [
            "Parallel Thinking",
            "Information Seeking Agents",
            "Agentic AI",
            "Reasoning Compression",
            "Exploration Efficiency",
            "Large Language Models"
          ],
          "read_time_minutes": 12
        },
        {
          "id": "15fa247fbf3673eb",
          "url": "http://arxiv.org/abs/2510.24688v1",
          "title": "MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection",
          "authors": [
            "Yun Zhang",
            "Zhaoliang Zheng",
            "Johnson Liu",
            "Zhiyu Huang",
            "Zewei Zhou",
            "Zonglin Meng",
            "Tianhui Cai",
            "Jiaqi Ma"
          ],
          "abstract": "Infrastructure-based perception plays a crucial role in intelligent\ntransportation systems, offering global situational awareness and enabling\ncooperative autonomy. However, existing camera-based detection models often\nunderperform in such scenarios due to challenges such as multi-view\ninfrastructure setup, diverse camera configurations, degraded visual inputs,\nand various road layouts. We introduce MIC-BEV, a Transformer-based\nbird's-eye-view (BEV) perception framework for infrastructure-based\nmulti-camera 3D object detection. MIC-BEV flexibly supports a variable number\nof cameras with heterogeneous intrinsic and extrinsic parameters and\ndemonstrates strong robustness under sensor degradation. The proposed\ngraph-enhanced fusion module in MIC-BEV integrates multi-view image features\ninto the BEV space by exploiting geometric relationships between cameras and\nBEV cells alongside latent visual cues. To support training and evaluation, we\nintroduce M2I, a synthetic dataset for infrastructure-based object detection,\nfeaturing diverse camera configurations, road layouts, and environmental\nconditions. Extensive experiments on both M2I and the real-world dataset\nRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D\nobject detection. It also remains robust under challenging conditions,\nincluding extreme weather and sensor degradation. These results highlight the\npotential of MIC-BEV for real-world deployment. The dataset and source code are\navailable at: https://github.com/HandsomeYun/MIC-BEV.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24688v1.pdf",
          "categories": [
            "cs.CV"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24688",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 47682,
            "final_url": "https://arxiv.org/abs/2510.24688v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24688v1] MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24688\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24688v1: MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24688v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"Infrastructure-based perception plays a crucial role in intelligent transportation systems, offering global situational awareness and enabling cooperative autonomy. However, existing camera-based detection models often underperform in such scenarios due to challenges such as multi-view infrastructure setup, diverse camera configurations, degraded visual inputs, and various road layouts. We introduce MIC-BEV, a Transformer-based bird's-eye-view (BEV) perception framework for infrastructure-based multi-camera 3D object detection. MIC-BEV flexibly supports a variable number of cameras with heterogeneous intrinsic and extrinsic parameters and demonstrates strong robustness under sensor degradation. The proposed graph-enhanced fusion module in MIC-BEV integrates multi-view image features into the BEV space by exploiting geometric relationships between cameras and BEV cells alongside latent visual cues. To support training and evaluation, we introduce M2I, a synthetic dataset for infrastructure-based object detection, featuring diverse camera configurations, road layouts, and environmental conditions. Extensive experiments on both M2I and the real-world dataset RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D object detection. It also remains robust under challenging conditions, including extreme weather and sensor degradation. These results highlight the potential of MIC-BEV for real-world deployment. The dataset and source code are available at: https://github.com/HandsomeYun/MIC-BEV.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View...\" name=\"twitter:title\"/>\n<meta content=\"Infrastructure-based perception plays a crucial role in intelligent transportation systems, offering global situational awareness and enabling cooperative autonomy. However, existing camera-based...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection\" name=\"citation_title\"/><meta content=\"Zhang, Yun\" name=\"citation_author\"/><meta content=\"Zheng, Zhaoliang\" name=\"citation_author\"/><meta content=\"Liu, Johnson\" name=\"citation_author\"/><meta content=\"Huang, Zhiyu\" name=\"citation_author\"/><meta content=\"Zhou, Zewei\" name=\"citation_author\"/><meta content=\"Meng, Zonglin\" name=\"citation_author\"/><meta content=\"Cai, Tianhui\" name=\"citation_author\"/><meta content=\"Ma, Jiaqi\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24688\" name=\"citation_pdf_url\"/><meta content=\"2510.24688\" name=\"citation_arxiv_id\"/><meta content=\"Infrastructure-based perception plays a crucial role in intelligent transportation systems, offering global situational awareness and enabling cooperative autonomy. However, existing camera-based detection models often underperform in such scenarios due to challenges such as multi-view infrastructure setup, diverse camera configurations, degraded visual inputs, and various road layouts. We introduce MIC-BEV, a Transformer-based bird's-eye-view (BEV) perception framework for infrastructure-based multi-camera 3D object detection. MIC-BEV flexibly supports a variable number of cameras with heterogeneous intrinsic and extrinsic parameters and demonstrates strong robustness under sensor degradation. The proposed graph-enhanced fusion module in MIC-BEV integrates multi-view image features into the BEV space by exploiting geometric relationships between cameras and BEV cells alongside latent visual cues. To support training and evaluation, we introduce M2I, a synthetic dataset for infrastructure-based object detection, featuring diverse camera configurations, road layouts, and environmental conditions. Extensive experiments on both M2I and the real-world dataset RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D object detection. It also remains robust under challenging conditions, including extreme weather and sensor degradation. These results highlight the potential of MIC-BEV for real-world deployment. The dataset and source code are available at: https://github.com/HandsomeYun/MIC-BEV.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24688v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option ",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": ""
        },
        {
          "id": "587bba2486feb568",
          "url": "http://arxiv.org/abs/2510.24692v1",
          "title": "Embodying Physical Computing into Soft Robots",
          "authors": [
            "Jun Wang",
            "Ziyang Zhou",
            "Ardalan Kahak",
            "Suyi Li"
          ],
          "abstract": "Softening and onboarding computers and controllers is one of the final\nfrontiers in soft robotics towards their robustness and intelligence for\neveryday use. In this regard, embodying soft and physical computing presents\nexciting potential. Physical computing seeks to encode inputs into a mechanical\ncomputing kernel and leverage the internal interactions among this kernel's\nconstituent elements to compute the output. Moreover, such input-to-output\nevolution can be re-programmable. This perspective paper proposes a framework\nfor embodying physical computing into soft robots and discusses three unique\nstrategies in the literature: analog oscillators, physical reservoir computing,\nand physical algorithmic computing. These embodied computers enable the soft\nrobot to perform complex behaviors that would otherwise require CMOS-based\nelectronics -- including coordinated locomotion with obstacle avoidance,\npayload weight and orientation classification, and programmable operation based\non logical rules. This paper will detail the working principles of these\nembodied physical computing methods, survey the current state-of-the-art, and\npresent a perspective for future development.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24692v1.pdf",
          "categories": [
            "cs.RO"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24692",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 45101,
            "final_url": "https://arxiv.org/abs/2510.24692v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24692v1] Embodying Physical Computing into Soft Robots</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24692\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24692v1: Embodying Physical Computing into Soft Robots\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"Embodying Physical Computing into Soft Robots\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24692v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"Softening and onboarding computers and controllers is one of the final frontiers in soft robotics towards their robustness and intelligence for everyday use. In this regard, embodying soft and physical computing presents exciting potential. Physical computing seeks to encode inputs into a mechanical computing kernel and leverage the internal interactions among this kernel's constituent elements to compute the output. Moreover, such input-to-output evolution can be re-programmable. This perspective paper proposes a framework for embodying physical computing into soft robots and discusses three unique strategies in the literature: analog oscillators, physical reservoir computing, and physical algorithmic computing. These embodied computers enable the soft robot to perform complex behaviors that would otherwise require CMOS-based electronics -- including coordinated locomotion with obstacle avoidance, payload weight and orientation classification, and programmable operation based on logical rules. This paper will detail the working principles of these embodied physical computing methods, survey the current state-of-the-art, and present a perspective for future development.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"Embodying Physical Computing into Soft Robots\" name=\"twitter:title\"/>\n<meta content=\"Softening and onboarding computers and controllers is one of the final frontiers in soft robotics towards their robustness and intelligence for everyday use. In this regard, embodying soft and...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"Embodying Physical Computing into Soft Robots\" name=\"citation_title\"/><meta content=\"Wang, Jun\" name=\"citation_author\"/><meta content=\"Zhou, Ziyang\" name=\"citation_author\"/><meta content=\"Kahak, Ardalan\" name=\"citation_author\"/><meta content=\"Li, Suyi\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24692\" name=\"citation_pdf_url\"/><meta content=\"2510.24692\" name=\"citation_arxiv_id\"/><meta content=\"Softening and onboarding computers and controllers is one of the final frontiers in soft robotics towards their robustness and intelligence for everyday use. In this regard, embodying soft and physical computing presents exciting potential. Physical computing seeks to encode inputs into a mechanical computing kernel and leverage the internal interactions among this kernel's constituent elements to compute the output. Moreover, such input-to-output evolution can be re-programmable. This perspective paper proposes a framework for embodying physical computing into soft robots and discusses three unique strategies in the literature: analog oscillators, physical reservoir computing, and physical algorithmic computing. These embodied computers enable the soft robot to perform complex behaviors that would otherwise require CMOS-based electronics -- including coordinated locomotion with obstacle avoidance, payload weight and orientation classification, and programmable operation based on logical rules. This paper will detail the working principles of these embodied physical computing methods, survey the current state-of-the-art, and present a perspective for future development.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24692v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option value=\"orcid\">ORCID</option>\n<option value=\"author_id\">arXiv author ID</option>\n<option value=\"help\">Help pages</option>\n<option value=\"full_text\">Full text</option>\n</select>\n</div>\n</div>\n<input name=\"source\" type=\"hidden\" value=\"header\"/>\n<button class=\"button is-small is-cul-darker\">Search</button>\n</div>\n</form>\n</div>\n</div><!-- /end desktop header -->\n<div class=\"mobile-header\">\n<div class=\"columns is-mobile\">\n<div class=\"column logo-arxiv\"><a href=\"https://arxiv.org/\"><img alt=\"arXiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logomark-small-white.svg\" style=\"height:60px;\"/></a></div>\n<div class=\"column logo-cornell\"><a href=\"https://www.cornell.edu/\">\n<picture>\n<source media=\"(min-width: 501px)\" sizes=\"400w\" srcset=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w\"/>\n<source srcset=\"/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x\"/>\n<img alt=\"Cornell University Logo\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/>\n</picture>\n</a></div>\n<div class=\"column nav\" id=\"toggle-container\" role=\"menubar\">\n<button class=\"toggle-control\"><svg class=\"icon filter-white\" viewbox=\"0 0 512 512\" xmlns=\"http://www.",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": ""
        },
        {
          "id": "f9b6b6bc00fa4acd",
          "url": "http://arxiv.org/abs/2510.24694v1",
          "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
          "authors": [
            "Yida Zhao",
            "Kuan Li",
            "Xixi Wu",
            "Liwen Zhang",
            "Dingchu Zhang",
            "Baixuan Li",
            "Maojia Song",
            "Zhuo Chen",
            "Chenxi Wang",
            "Xinyu Wang",
            "Kewei Tu",
            "Pengjun Xie",
            "Jingren Zhou",
            "Yong Jiang"
          ],
          "abstract": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24694v1.pdf",
          "categories": [
            "cs.CL",
            "cs.AI"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24694",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 48124,
            "final_url": "https://arxiv.org/abs/2510.24694v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24694v1] Repurposing Synthetic Data for Fine-grained Search Agent Supervision</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24694\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24694v1: Repurposing Synthetic Data for Fine-grained Search Agent Supervision\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"Repurposing Synthetic Data for Fine-grained Search Agent Supervision\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24694v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative &quot;near-miss&quot; samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent's reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these &quot;near-misses&quot;. Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"Repurposing Synthetic Data for Fine-grained Search Agent Supervision\" name=\"twitter:title\"/>\n<meta content=\"LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"Repurposing Synthetic Data for Fine-grained Search Agent Supervision\" name=\"citation_title\"/><meta content=\"Zhao, Yida\" name=\"citation_author\"/><meta content=\"Li, Kuan\" name=\"citation_author\"/><meta content=\"Wu, Xixi\" name=\"citation_author\"/><meta content=\"Zhang, Liwen\" name=\"citation_author\"/><meta content=\"Zhang, Dingchu\" name=\"citation_author\"/><meta content=\"Li, Baixuan\" name=\"citation_author\"/><meta content=\"Song, Maojia\" name=\"citation_author\"/><meta content=\"Chen, Zhuo\" name=\"citation_author\"/><meta content=\"Wang, Chenxi\" name=\"citation_author\"/><meta content=\"Wang, Xinyu\" name=\"citation_author\"/><meta content=\"Tu, Kewei\" name=\"citation_author\"/><meta content=\"Xie, Pengjun\" name=\"citation_author\"/><meta content=\"Zhou, Jingren\" name=\"citation_author\"/><meta content=\"Jiang, Yong\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24694\" name=\"citation_pdf_url\"/><meta content=\"2510.24694\" name=\"citation_arxiv_id\"/><meta content=\"LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative &quot;near-miss&quot; samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent's reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these &quot;near-misses&quot;. Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24694v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"r",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "Entity-Aware Rewards Enhance LLM Search Agent Supervision",
          "tldr": "Current training methods for LLM-based search agents often discard valuable entity information, hindering learning from partially correct responses. This research introduces Entity-aware Group Relative Policy Optimization (E-GRPO), which leverages a dense entity-aware reward function to assign partial credit to 'near-miss' samples. E-GRPO significantly outperforms existing baselines, leading to more accurate and efficient search agent reasoning.",
          "bullets": [
            "Prevailing LLM search agent training methods, like GRPO, discard rich entity information, missing valuable learning signals from 'near-miss' samples.",
            "Empirical analysis reveals a strong positive correlation between identified ground-truth entities during an agent's reasoning process and final answer accuracy.",
            "The novel Entity-aware Group Relative Policy Optimization (E-GRPO) framework introduces a dense entity-aware reward function.",
            "E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling learning from partially correct reasoning.",
            "Experiments on diverse question-answering (QA) and deep research benchmarks demonstrate that E-GRPO consistently and significantly outperforms the GRPO baseline.",
            "E-GRPO achieves superior accuracy and induces more efficient reasoning policies, requiring fewer tool calls.",
            "This approach offers a more effective and sample-efficient method for aligning LLM-based search agents."
          ],
          "significance": "This work significantly advances the training of LLM-based search agents by introducing a more granular reward mechanism. By learning from 'near-misses' rather than just final outcomes, agents can develop more robust and efficient reasoning policies. This has substantial implications for improving performance on complex, knowledge-intensive tasks across various domains.",
          "limitations": "The abstract does not explicitly detail potential computational overhead associated with entity extraction and reward calculation, nor does it discuss the generalizability of E-GRPO to domains outside of question-answering and deep research benchmarks.",
          "keywords": [
            "LLM search agents",
            "entity-aware rewards",
            "reinforcement learning",
            "policy optimization",
            "synthetic data",
            "question answering",
            "reasoning efficiency"
          ],
          "read_time_minutes": 25
        },
        {
          "id": "95567acd3a8a30db",
          "url": "http://arxiv.org/abs/2510.24693v1",
          "title": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence",
          "authors": [
            "Zihan Liu",
            "Zhikang Niu",
            "Qiuyang Xiao",
            "Zhisheng Zheng",
            "Ruoqi Yuan",
            "Yuhang Zang",
            "Yuhang Cao",
            "Xiaoyi Dong",
            "Jianze Liang",
            "Xie Chen",
            "Leilei Sun",
            "Dahua Lin",
            "Jiaqi Wang"
          ],
          "abstract": "Despite rapid progress in Multi-modal Large Language Models and Large\nAudio-Language Models, existing audio benchmarks largely test semantics that\ncan be recovered from text captions, masking deficits in fine-grained\nperceptual reasoning. We formalize audio 4D intelligence that is defined as\nreasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to\nmeasure it. STAR-Bench combines a Foundational Acoustic Perception setting (six\nattributes under absolute and relative regimes) with a Holistic Spatio-Temporal\nReasoning setting that includes segment reordering for continuous and discrete\nprocesses and spatial tasks spanning static localization, multi-source\nrelations, and dynamic trajectories. Our data curation pipeline uses two\nmethods to ensure high-quality samples. For foundational tasks, we use\nprocedurally synthesized and physics-simulated audio. For holistic data, we\nfollow a four-stage process that includes human annotation and final selection\nbased on human performance. Unlike prior benchmarks where caption-only\nanswering reduces accuracy slightly, STAR-Bench induces far larger drops\n(-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically\nhard-to-describe cues. Evaluating 19 models reveals substantial gaps compared\nwith humans and a capability hierarchy: closed-source models are bottlenecked\nby fine-grained perception, while open-source models lag across perception,\nknowledge, and reasoning. Our STAR-Bench provides critical insights and a clear\npath forward for developing future models with a more robust understanding of\nthe physical world.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24693v1.pdf",
          "categories": [
            "cs.SD",
            "cs.CL",
            "eess.AS"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24693",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 48814,
            "final_url": "https://arxiv.org/abs/2510.24693v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24693v1] STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24693\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24693v1: STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24693v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning. We formalize audio 4D intelligence that is defined as reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories. Our data curation pipeline uses two methods to ensure high-quality samples. For foundational tasks, we use procedurally synthesized and physics-simulated audio. For holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance. Unlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\" name=\"twitter:title\"/>\n<meta content=\"Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\" name=\"citation_title\"/><meta content=\"Liu, Zihan\" name=\"citation_author\"/><meta content=\"Niu, Zhikang\" name=\"citation_author\"/><meta content=\"Xiao, Qiuyang\" name=\"citation_author\"/><meta content=\"Zheng, Zhisheng\" name=\"citation_author\"/><meta content=\"Yuan, Ruoqi\" name=\"citation_author\"/><meta content=\"Zang, Yuhang\" name=\"citation_author\"/><meta content=\"Cao, Yuhang\" name=\"citation_author\"/><meta content=\"Dong, Xiaoyi\" name=\"citation_author\"/><meta content=\"Liang, Jianze\" name=\"citation_author\"/><meta content=\"Chen, Xie\" name=\"citation_author\"/><meta content=\"Sun, Leilei\" name=\"citation_author\"/><meta content=\"Lin, Dahua\" name=\"citation_author\"/><meta content=\"Wang, Jiaqi\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24693\" name=\"citation_pdf_url\"/><meta content=\"2510.24693\" name=\"citation_arxiv_id\"/><meta content=\"Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning. We formalize audio 4D intelligence that is defined as reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories. Our data curation pipeline uses two methods to ensure high-quality samples. For foundational tasks, we use procedurally synthesized and physics-simulated audio. For holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance. Unlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24693v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "STAR-Bench Measures Audio 4D Intelligence, Reveals Model Deficits in Spatio-Temporal Reasoning",
          "tldr": "Existing audio benchmarks overlook fine-grained perceptual reasoning. This paper introduces STAR-Bench, a new benchmark formalizing \"audio 4D intelligence\" to measure reasoning over sound dynamics in time and 3D space. Evaluations show current models experience significant performance drops (-31.5% temporal, -35.2% spatial) compared to humans, highlighting substantial deficits in spatio-temporal understanding.",
          "bullets": [
            "Audio 4D intelligence is formalized as reasoning over sound dynamics in time and 3D space, which STAR-Bench aims to measure.",
            "STAR-Bench includes a Foundational Acoustic Perception setting with six attributes and a Holistic Spatio-Temporal Reasoning setting covering segment reordering and spatial tasks.",
            "Data curation for foundational tasks uses procedurally synthesized and physics-simulated audio, while holistic data involves a four-stage human annotation process.",
            "Caption-only answering on STAR-Bench leads to significant performance drops for models (-31.5% temporal, -35.2% spatial), indicating its focus on linguistically hard-to-describe cues.",
            "Evaluation of 19 models reveals substantial performance gaps compared to human capabilities on the benchmark.",
            "A capability hierarchy shows closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning."
          ],
          "significance": "STAR-Bench addresses a critical gap in evaluating advanced audio intelligence by focusing on linguistically hard-to-describe spatio-temporal cues. This benchmark offers a standardized method to assess how well models understand the physical world through sound, guiding the development of more robust and perceptually aware AI systems. It provides a clear path forward for future research.",
          "limitations": "The study reveals current models exhibit substantial performance gaps compared to humans, indicating significant work remains to achieve robust audio 4D intelligence. Future research will focus on developing models capable of closing these identified deficiencies.",
          "keywords": [
            "Audio 4D intelligence",
            "Spatio-Temporal Reasoning",
            "Audio Benchmarks",
            "Multi-modal LLMs",
            "Sound Dynamics",
            "Perceptual Reasoning"
          ],
          "read_time_minutes": 15
        },
        {
          "id": "e13c77e8c12cbc5e",
          "url": "http://arxiv.org/abs/2510.24699v1",
          "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
          "authors": [
            "Rui Ye",
            "Zhongwang Zhang",
            "Kuan Li",
            "Huifeng Yin",
            "Zhengwei Tao",
            "Yida Zhao",
            "Liangcai Su",
            "Liwen Zhang",
            "Zile Qiao",
            "Xinyu Wang",
            "Pengjun Xie",
            "Fei Huang",
            "Siheng Chen",
            "Jingren Zhou",
            "Yong Jiang"
          ],
          "abstract": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24699v1.pdf",
          "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24699",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 48153,
            "final_url": "https://arxiv.org/abs/2510.24699v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24699v1] AgentFold: Long-Horizon Web Agents with Proactive Context Management</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24699\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24699v1: AgentFold: Long-Horizon Web Agents with Proactive Context Management\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"AgentFold: Long-Horizon Web Agents with Proactive Context Management\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24699v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"AgentFold: Long-Horizon Web Agents with Proactive Context Management\" name=\"twitter:title\"/>\n<meta content=\"LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"AgentFold: Long-Horizon Web Agents with Proactive Context Management\" name=\"citation_title\"/><meta content=\"Ye, Rui\" name=\"citation_author\"/><meta content=\"Zhang, Zhongwang\" name=\"citation_author\"/><meta content=\"Li, Kuan\" name=\"citation_author\"/><meta content=\"Yin, Huifeng\" name=\"citation_author\"/><meta content=\"Tao, Zhengwei\" name=\"citation_author\"/><meta content=\"Zhao, Yida\" name=\"citation_author\"/><meta content=\"Su, Liangcai\" name=\"citation_author\"/><meta content=\"Zhang, Liwen\" name=\"citation_author\"/><meta content=\"Qiao, Zile\" name=\"citation_author\"/><meta content=\"Wang, Xinyu\" name=\"citation_author\"/><meta content=\"Xie, Pengjun\" name=\"citation_author\"/><meta content=\"Huang, Fei\" name=\"citation_author\"/><meta content=\"Chen, Siheng\" name=\"citation_author\"/><meta content=\"Zhou, Jingren\" name=\"citation_author\"/><meta content=\"Jiang, Yong\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24699\" name=\"citation_pdf_url\"/><meta content=\"2510.24699\" name=\"citation_arxiv_id\"/><meta content=\"LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24699v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option value=\"orcid\">ORCID</option>\n<option value=\"author_id\">arXiv author ID</option>\n<option value=\"help\">Help pages</option>",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "AgentFold Boosts Web Agents with Proactive Context Management",
          "tldr": "AgentFold introduces a novel paradigm for LLM-based web agents, addressing the critical context management trade-off in long-horizon tasks. Inspired by human cognition, it proactively manages historical trajectories through multi-scale 'folding' operations. This approach significantly improves performance, with AgentFold-30B-A3B surpassing larger open-source and leading proprietary models on prominent benchmarks.",
          "bullets": [
            "LLM-based web agents struggle with context saturation or critical detail loss on long-horizon tasks due to current context management strategies.",
            "AgentFold proposes a novel agent paradigm centered on proactive context management, treating context as a dynamic cognitive workspace.",
            "It employs a 'folding' operation to manage historical trajectories at multiple scales, performing granular condensations or deep consolidations.",
            "This method is inspired by the human cognitive process of retrospective consolidation.",
            "AgentFold-30B-A3B achieved 36.2% on BrowseComp and 47.3% on BrowseComp-ZH benchmarks.",
            "Its performance surpasses or matches open-source models of dramatically larger scale and leading proprietary agents like OpenAI's o4-mini.",
            "The results were achieved using simple supervised fine-tuning, without continual pre-training or reinforcement learning."
          ],
          "significance": "AgentFold's proactive context management offers a critical advancement for LLM-based web agents, enabling them to handle complex, long-horizon tasks more effectively. By preventing context saturation and detail loss, it significantly improves agent reliability and performance, potentially expanding the practical applications of AI in web interaction and information seeking.",
          "limitations": "The provided abstract does not detail specific limitations or caveats of the AgentFold approach.",
          "keywords": [
            "LLM web agents",
            "context management",
            "long-horizon tasks",
            "AgentFold",
            "proactive context",
            "cognitive workspace",
            "benchmarks"
          ],
          "read_time_minutes": 15
        },
        {
          "id": "0ab18271632e5153",
          "url": "http://arxiv.org/abs/2510.24687v1",
          "title": "Fast algorithms enabling optimization and deep learning for photoacoustic tomography in a circular detection geometry",
          "authors": [
            "Andreas Hauptmann",
            "Leonid Kunyansky",
            "Jenni Poimala"
          ],
          "abstract": "The inverse source problem arising in photoacoustic tomography and in several\nother coupled-physics modalities is frequently solved by iterative algorithms.\nSuch algorithms are based on the minimization of a certain cost functional. In\naddition, novel deep learning techniques are currently being investigated to\nfurther improve such optimization approaches. All such methods require multiple\napplications of the operator defining the forward problem, and of its adjoint.\nIn this paper, we present new asymptotically fast algorithms for numerical\nevaluation of the forward and adjoint operators, applicable in the circular\nacquisition geometry. For an $(n \\times n)$ image, our algorithms compute these\noperators in $\\mathcal{O}(n^2 \\log n)$ floating point operations. We\ndemonstrate the performance of our algorithms in numerical simulations, where\nthey are used as an integral part of several iterative image reconstruction\ntechniques: classic variational methods, such as non-negative least squares and\ntotal variation regularized least squares, as well as deep learning methods,\nsuch as learned primal dual. A Python implementation of our algorithms and\ncomputational examples is available to the general public.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24687v1.pdf",
          "categories": [
            "eess.IV",
            "cs.AI",
            "cs.NA",
            "math.AP",
            "math.NA",
            "math.OC"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24687",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 46838,
            "final_url": "https://arxiv.org/abs/2510.24687v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24687v1] Fast algorithms enabling optimization and deep learning for photoacoustic tomography in a circular detection geometry</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24687\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24687v1: Fast algorithms enabling optimization and deep learning for photoacoustic tomography in a circular detection geometry\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"Fast algorithms enabling optimization and deep learning for photoacoustic tomography in a circular detection geometry\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24687v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"The inverse source problem arising in photoacoustic tomography and in several other coupled-physics modalities is frequently solved by iterative algorithms. Such algorithms are based on the minimization of a certain cost functional. In addition, novel deep learning techniques are currently being investigated to further improve such optimization approaches. All such methods require multiple applications of the operator defining the forward problem, and of its adjoint. In this paper, we present new asymptotically fast algorithms for numerical evaluation of the forward and adjoint operators, applicable in the circular acquisition geometry. For an $(n \\times n)$ image, our algorithms compute these operators in $\\mathcal{O}(n^2 \\log n)$ floating point operations. We demonstrate the performance of our algorithms in numerical simulations, where they are used as an integral part of several iterative image reconstruction techniques: classic variational methods, such as non-negative least squares and total variation regularized least squares, as well as deep learning methods, such as learned primal dual. A Python implementation of our algorithms and computational examples is available to the general public.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"Fast algorithms enabling optimization and deep learning for...\" name=\"twitter:title\"/>\n<meta content=\"The inverse source problem arising in photoacoustic tomography and in several other coupled-physics modalities is frequently solved by iterative algorithms. Such algorithms are based on the...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"Fast algorithms enabling optimization and deep learning for photoacoustic tomography in a circular detection geometry\" name=\"citation_title\"/><meta content=\"Hauptmann, Andreas\" name=\"citation_author\"/><meta content=\"Kunyansky, Leonid\" name=\"citation_author\"/><meta content=\"Poimala, Jenni\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24687\" name=\"citation_pdf_url\"/><meta content=\"2510.24687\" name=\"citation_arxiv_id\"/><meta content=\"The inverse source problem arising in photoacoustic tomography and in several other coupled-physics modalities is frequently solved by iterative algorithms. Such algorithms are based on the minimization of a certain cost functional. In addition, novel deep learning techniques are currently being investigated to further improve such optimization approaches. All such methods require multiple applications of the operator defining the forward problem, and of its adjoint. In this paper, we present new asymptotically fast algorithms for numerical evaluation of the forward and adjoint operators, applicable in the circular acquisition geometry. For an $(n \\times n)$ image, our algorithms compute these operators in $\\mathcal{O}(n^2 \\log n)$ floating point operations. We demonstrate the performance of our algorithms in numerical simulations, where they are used as an integral part of several iterative image reconstruction techniques: classic variational methods, such as non-negative least squares and total variation regularized least squares, as well as deep learning methods, such as learned primal dual. A Python implementation of our algorithms and computational examples is available to the general public.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/eess/recent\">eess</a> <span>&gt;</span> arXiv:2510.24687v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option value=\"orcid\">ORCID</option>\n<option value=\"author_id\">arXiv author ID</option>\n<option value=\"help\">Help pages</option>\n<option value=\"full_text\">Full text</option>\n</select>\n</div>\n</div>\n<input name=\"source\" type=\"hidden\" value=\"header\"/>\n<button class=\"button is-small is-cul-darker\">Search</button>\n</div>\n</form>\n</div>\n</div><!-- /end desktop header -->\n<div class=\"mobile-header\">\n<div class=\"columns is-mobile\">\n<div class=\"column logo-arxiv\"><a href=\"https://arxiv.org/\"><img alt=\"arXiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logomark-small-white.svg\" style=\"height:60px;\"/></a></div>\n<div class=\"column logo-cornell\"><a href=\"https://www.cornell.edu/\">\n<picture>\n<source media=\"(min-width: 501px)\" sizes=\"400w\" srcset=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w\"/>\n<source srcset=\"/static/browse/0.3.4/images/icons/cu/cornell",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "New Fast Algorithms Accelerate Photoacoustic Tomography Image Reconstruction",
          "tldr": "This research introduces novel, asymptotically fast algorithms for the forward and adjoint operators in photoacoustic tomography, specifically for circular detection geometries. These algorithms compute operators for an (n x n) image in $\\mathcal{O}(n^2 \\log n)$ operations, significantly speeding up iterative image reconstruction methods, including both classic variational and deep learning approaches.",
          "bullets": [
            "Presents new asymptotically fast algorithms for photoacoustic tomography's forward and adjoint operators.",
            "Designed for application in a circular acquisition geometry.",
            "Achieves computational complexity of $\\mathcal{O}(n^2 \\log n)$ floating point operations for an $(n \\times n)$ image.",
            "Demonstrated performance in numerical simulations across various iterative image reconstruction techniques.",
            "Integrates with classic variational methods like non-negative least squares and total variation regularized least squares.",
            "Also supports deep learning methods, exemplified by learned primal dual.",
            "A Python implementation and computational examples are publicly available."
          ],
          "significance": "These faster algorithms are crucial for advancing photoacoustic tomography, enabling more efficient and complex image reconstruction. By reducing computational time, they facilitate the development and application of sophisticated deep learning and variational methods, potentially leading to higher quality images and broader clinical or research utility.",
          "limitations": "The abstract does not detail specific limitations, but the presented algorithms are designed for a circular acquisition geometry, suggesting their direct applicability might be constrained to this specific setup.",
          "keywords": [
            "Photoacoustic Tomography",
            "Fast Algorithms",
            "Inverse Problems",
            "Deep Learning",
            "Image Reconstruction",
            "Circular Geometry",
            "Computational Efficiency"
          ],
          "read_time_minutes": 45
        },
        {
          "id": "6543696b3cd726f2",
          "url": "http://arxiv.org/abs/2510.24677v1",
          "title": "Dissecting Role Cognition in Medical LLMs via Neuronal Ablation",
          "authors": [
            "Xun Liang",
            "Huayi Lai",
            "Hanyu Wang",
            "Wentao Zhang",
            "Linfeng Zhang",
            "Yanfang Chen",
            "Feiyu Xiong",
            "Zhiyu Li"
          ],
          "abstract": "Large language models (LLMs) have gained significant traction in medical\ndecision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common\npractice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students,\nresidents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model\nreasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to\nevaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style.\nWe test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to\nassess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the\nmedical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of\ndistinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core\ndecision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the\ncognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and\nemphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the\nrelated code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24677v1.pdf",
          "categories": [
            "cs.CL",
            "cs.AI"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24677",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 47766,
            "final_url": "https://arxiv.org/abs/2510.24677v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24677v1] Dissecting Role Cognition in Medical LLMs via Neuronal Ablation</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24677\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24677v1: Dissecting Role Cognition in Medical LLMs via Neuronal Ablation\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"Dissecting Role Cognition in Medical LLMs via Neuronal Ablation\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24677v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"Large language models (LLMs) have gained significant traction in medical decision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common practice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students, residents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model reasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to evaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style. We test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to assess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the medical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of distinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core decision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the cognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and emphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the related code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"Dissecting Role Cognition in Medical LLMs via Neuronal Ablation\" name=\"twitter:title\"/>\n<meta content=\"Large language models (LLMs) have gained significant traction in medical decision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"Dissecting Role Cognition in Medical LLMs via Neuronal Ablation\" name=\"citation_title\"/><meta content=\"Liang, Xun\" name=\"citation_author\"/><meta content=\"Lai, Huayi\" name=\"citation_author\"/><meta content=\"Wang, Hanyu\" name=\"citation_author\"/><meta content=\"Zhang, Wentao\" name=\"citation_author\"/><meta content=\"Zhang, Linfeng\" name=\"citation_author\"/><meta content=\"Chen, Yanfang\" name=\"citation_author\"/><meta content=\"Xiong, Feiyu\" name=\"citation_author\"/><meta content=\"Li, Zhiyu\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24677\" name=\"citation_pdf_url\"/><meta content=\"2510.24677\" name=\"citation_arxiv_id\"/><meta content=\"Large language models (LLMs) have gained significant traction in medical decision support systems, particularly in the context of medical question answering and role-playing simulations. A common practice, Prompt-Based Role Playing (PBRP), instructs models to adopt different clinical roles (e.g., medical students, residents, attending physicians) to simulate varied professional behaviors. However, the impact of such role prompts on model reasoning capabilities remains unclear. This study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to evaluate whether role prompts induce distinct, role-specific cognitive processes in LLMs or merely modify linguistic style. We test this framework on three medical QA datasets, employing neuron ablation and representation analysis techniques to assess changes in reasoning pathways. Our results demonstrate that role prompts do not significantly enhance the medical reasoning abilities of LLMs. Instead, they primarily affect surface-level linguistic features, with no evidence of distinct reasoning pathways or cognitive differentiation across clinical roles. Despite superficial stylistic changes, the core decision-making mechanisms of LLMs remain uniform across roles, indicating that current PBRP methods fail to replicate the cognitive complexity found in real-world medical practice. This highlights the limitations of role-playing in medical AI and emphasizes the need for models that simulate genuine cognitive processes rather than linguistic imitation.We have released the related code in the following repository:https: //github.com/IAAR-Shanghai/RolePlay_LLMDoctor\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24677v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">D",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "Medical LLM Role Prompts Lack Cognitive Depth, Only Alter Style",
          "tldr": "This study introduces the RP-Neuron-Activated Evaluation Framework (RPNA) to assess if role prompts induce distinct cognitive processes in medical LLMs. Using neuron ablation on three medical QA datasets, researchers found that role prompts primarily affect linguistic style, not enhancing reasoning abilities. The core decision-making mechanisms remain uniform, indicating current prompt-based role-playing fails to replicate genuine cognitive complexity.",
          "bullets": [
            "Introduced the RP-Neuron-Activated Evaluation Framework (RPNA) to analyze the impact of role prompts on LLM cognition.",
            "Evaluated medical LLMs on three distinct medical question-answering datasets.",
            "Employed neuron ablation and representation analysis techniques to assess changes in reasoning pathways.",
            "Demonstrated that role prompts do not significantly enhance LLMs' medical reasoning capabilities.",
            "Found that role prompts primarily affect surface-level linguistic features, not creating distinct cognitive differentiation.",
            "Concluded that core decision-making mechanisms of LLMs remain uniform across different simulated roles.",
            "Highlighted that current Prompt-Based Role Playing (PBRP) methods fail to replicate real-world medical cognitive complexity."
          ],
          "significance": "This research is significant as it challenges the assumption that role prompts improve medical LLMs' reasoning. It reveals that current role-playing methods offer only superficial stylistic changes, underscoring the need for developing models that can simulate genuine cognitive processes rather than mere linguistic imitation for robust medical AI.",
          "limitations": "The study indicates that current Prompt-Based Role Playing (PBRP) methods are limited in their ability to induce genuine cognitive differentiation in medical LLMs, failing to replicate real-world medical practice complexity.",
          "keywords": [
            "Medical LLMs",
            "Role Playing",
            "Neuron Ablation",
            "Cognitive Processes",
            "Prompt Engineering",
            "Medical AI",
            "Reasoning"
          ],
          "read_time_minutes": 15
        }
      ]
    }
  ],
  "metadata": {
    "generated_at": "2025-10-29T17:46:51.779954",
    "sources_count": 1,
    "papers_count": 15,
    "news_count": 0,
    "total_items": 15
  }
}