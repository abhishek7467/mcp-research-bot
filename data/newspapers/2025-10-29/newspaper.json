{
  "date": "2025-10-29",
  "title": "Daily Research Bulletin: quantum computing",
  "topics": [
    "quantum computing"
  ],
  "editorial": "Today's highlights in quantum computing:  (arXiv); URSA Bridges Discrete-Continuous Gap for Scalable Video Generation (arXiv); Generative View Stitching Enables Collision-Free Camera-Guided Video Generation (arXiv).",
  "sections": [
    {
      "id": "top_research",
      "title": "Top Research Picks",
      "items": [
        {
          "id": "1721203ce5b4fa7f",
          "url": "http://arxiv.org/abs/2510.24715v1",
          "title": "On the Field Excursion Bound",
          "authors": [
            "Tom Rudelius"
          ],
          "abstract": "In a recent work, Herderschee and Wall (HW) proved a bound on scalar field\nexcursions in spatially flat FRW cosmologies. In this note, we give an\nalternate proof of their bound using the Friedmann equations, and we prove that\nit can be saturated only in universes with vanishing acceleration, $\\ddot a\n=0$. We argue that in a realistic (eternal) inflation scenario, the bound is\nrobust against quantum corrections and spacetime curvature, and it can be\nviolated by higher-derivative corrections only at the expense of a superluminal\nspeed of sound. We further speculate on possible connections between the\nswampland program and the vacuum estimates given in the work of HW.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24715v1.pdf",
          "categories": [
            "hep-th",
            "gr-qc"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24715",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 44207,
            "final_url": "https://arxiv.org/abs/2510.24715v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24715v1] On the Field Excursion Bound</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24715\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24715v1: On the Field Excursion Bound\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"On the Field Excursion Bound\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24715v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"In a recent work, Herderschee and Wall (HW) proved a bound on scalar field excursions in spatially flat FRW cosmologies. In this note, we give an alternate proof of their bound using the Friedmann equations, and we prove that it can be saturated only in universes with vanishing acceleration, $\\ddot a =0$. We argue that in a realistic (eternal) inflation scenario, the bound is robust against quantum corrections and spacetime curvature, and it can be violated by higher-derivative corrections only at the expense of a superluminal speed of sound. We further speculate on possible connections between the swampland program and the vacuum estimates given in the work of HW.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"On the Field Excursion Bound\" name=\"twitter:title\"/>\n<meta content=\"In a recent work, Herderschee and Wall (HW) proved a bound on scalar field excursions in spatially flat FRW cosmologies. In this note, we give an alternate proof of their bound using the Friedmann...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"On the Field Excursion Bound\" name=\"citation_title\"/><meta content=\"Rudelius, Tom\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24715\" name=\"citation_pdf_url\"/><meta content=\"2510.24715\" name=\"citation_arxiv_id\"/><meta content=\"In a recent work, Herderschee and Wall (HW) proved a bound on scalar field excursions in spatially flat FRW cosmologies. In this note, we give an alternate proof of their bound using the Friedmann equations, and we prove that it can be saturated only in universes with vanishing acceleration, $\\ddot a =0$. We argue that in a realistic (eternal) inflation scenario, the bound is robust against quantum corrections and spacetime curvature, and it can be violated by higher-derivative corrections only at the expense of a superluminal speed of sound. We further speculate on possible connections between the swampland program and the vacuum estimates given in the work of HW.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/hep-th/recent\">hep-th</a> <span>&gt;</span> arXiv:2510.24715v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option value=\"orcid\">ORCID</option>\n<option value=\"author_id\">arXiv author ID</option>\n<option value=\"help\">Help pages</option>\n<option value=\"full_text\">Full text</option>\n</select>\n</div>\n</div>\n<input name=\"source\" type=\"hidden\" value=\"header\"/>\n<button class=\"button is-small is-cul-darker\">Search</button>\n</div>\n</form>\n</div>\n</div><!-- /end desktop header -->\n<div class=\"mobile-header\">\n<div class=\"columns is-mobile\">\n<div class=\"column logo-arxiv\"><a href=\"https://arxiv.org/\"><img alt=\"arXiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logomark-small-white.svg\" style=\"height:60px;\"/></a></div>\n<div class=\"column logo-cornell\"><a href=\"https://www.cornell.edu/\">\n<picture>\n<source media=\"(min-width: 501px)\" sizes=\"400w\" srcset=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w\"/>\n<source srcset=\"/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x\"/>\n<img alt=\"Cornell University Logo\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/>\n</picture>\n</a></div>\n<div class=\"column nav\" id=\"toggle-container\" role=\"menubar\">\n<button class=\"toggle-control\"><svg class=\"icon filter-white\" viewbox=\"0 0 512 512\" xmlns=\"http://www.w3.org/2000/svg\"><title>open search</title><path d=\"M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z\"></path></svg></button>\n<div class=\"mobile-toggle-block toggle-target\">\n<form action=\"https://arxiv.org/search\" class=\"mobile-search-form\" method=\"GET\">\n<div class=\"field has-addons\">\n<input aria-label=\"Search term or terms\" class=\"input\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<input name=\"source\" type=\"hidden\" value=\"header\"/>\n<input name=\"searchtype\" type=\"hidden\" value=\"all\"/>\n<button class=\"button\">GO</button>\n</div>\n</form>\n</div>\n<button class=\"toggle-control\"><svg class=\"icon filter-white\" role=\"menu\" viewbox=\"0 0 448 512\" xmlns=\"http://www.w3.org/2000/svg\"><title>open navigation menu</title><path d=\"M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 1",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": ""
        },
        {
          "id": "d9f8e23b4b916d05",
          "url": "http://arxiv.org/abs/2510.24717v1",
          "title": "Uniform Discrete Diffusion with Metric Path for Video Generation",
          "authors": [
            "Haoge Deng",
            "Ting Pan",
            "Fan Zhang",
            "Yang Liu",
            "Zhuoyan Luo",
            "Yufeng Cui",
            "Wenxuan Wang",
            "Chunhua Shen",
            "Shiguang Shan",
            "Zhaoxiang Zhang",
            "Xinlong Wang"
          ],
          "abstract": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24717v1.pdf",
          "categories": [
            "cs.CV"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24717",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 46999,
            "final_url": "https://arxiv.org/abs/2510.24717v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24717v1] Uniform Discrete Diffusion with Metric Path for Video Generation</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24717\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24717v1: Uniform Discrete Diffusion with Metric Path for Video Generation\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"Uniform Discrete Diffusion with Metric Path for Video Generation\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24717v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"Uniform Discrete Diffusion with Metric Path for Video Generation\" name=\"twitter:title\"/>\n<meta content=\"Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"Uniform Discrete Diffusion with Metric Path for Video Generation\" name=\"citation_title\"/><meta content=\"Deng, Haoge\" name=\"citation_author\"/><meta content=\"Pan, Ting\" name=\"citation_author\"/><meta content=\"Zhang, Fan\" name=\"citation_author\"/><meta content=\"Liu, Yang\" name=\"citation_author\"/><meta content=\"Luo, Zhuoyan\" name=\"citation_author\"/><meta content=\"Cui, Yufeng\" name=\"citation_author\"/><meta content=\"Wang, Wenxuan\" name=\"citation_author\"/><meta content=\"Shen, Chunhua\" name=\"citation_author\"/><meta content=\"Shan, Shiguang\" name=\"citation_author\"/><meta content=\"Zhang, Zhaoxiang\" name=\"citation_author\"/><meta content=\"Wang, Xinlong\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24717\" name=\"citation_pdf_url\"/><meta content=\"2510.24717\" name=\"citation_arxiv_id\"/><meta content=\"Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24717v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option value=\"orcid\">ORCID</option>\n<option value=\"author_id\">arXiv author ID</option>\n<option value=\"help\">Help pages</option>\n<option value=\"full_text\">Full text</option>\n</select>\n</div>\n</div>\n<input name=\"source\" type=\"hidden\" value=\"header\"/>\n<button class=\"button is-small is-cul-darker\">Search</button>\n</div>\n</form>\n</div>\n</div><!-- /end desktop header -->\n<div class=\"mobile-header\">\n<div class=\"columns is-mobile\">\n<div class=\"column logo-arxiv\"><a href=\"https://arxiv.org/\"><img alt=\"arXiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logomark-small-whi",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "URSA Bridges Discrete-Continuous Gap for Scalable Video Generation",
          "tldr": "Researchers introduce URSA, a novel discrete diffusion framework that addresses error accumulation and inconsistency in video generation. By formulating video generation as iterative global refinement of spatiotemporal tokens, URSA achieves performance comparable to state-of-the-art continuous methods. This framework enables efficient, scalable high-resolution image and long-duration video synthesis with significantly fewer inference steps.",
          "bullets": [
            "URSA is a discrete diffusion framework designed to overcome limitations of previous discrete video generation methods, specifically error accumulation and long-context inconsistency.",
            "The framework formulates video generation as an iterative global refinement process of discrete spatiotemporal tokens.",
            "It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism.",
            "These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation.",
            "URSA requires significantly fewer inference steps compared to existing methods.",
            "An asynchronous temporal fine-tuning strategy unifies versatile tasks, including interpolation and image-to-video generation, within a single model.",
            "Extensive experiments demonstrate URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods."
          ],
          "significance": "This work significantly advances discrete generative modeling for video, a domain previously lagging behind continuous approaches. URSA's ability to scale efficiently and perform comparably to state-of-the-art continuous methods could democratize high-quality video generation, requiring fewer computational resources and inference steps. Its unified task capability also streamlines development for various video-related applications.",
          "limitations": "The provided abstract does not explicitly detail specific limitations or areas for future work beyond its current capabilities.",
          "keywords": [
            "Discrete Diffusion",
            "Video Generation",
            "Generative Models",
            "Spatiotemporal Tokens",
            "Deep Learning",
            "Metric Path",
            "URSA"
          ],
          "read_time_minutes": 25
        },
        {
          "id": "28b25ecc02a17d94",
          "url": "http://arxiv.org/abs/2510.24718v1",
          "title": "Generative View Stitching",
          "authors": [
            "Chonghyuk Song",
            "Michal Stary",
            "Boyuan Chen",
            "George Kopanas",
            "Vincent Sitzmann"
          ],
          "abstract": "Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.",
          "published_at": "2025-10-28",
          "source": "arXiv",
          "type": "research",
          "pdf_url": "http://arxiv.org/pdf/2510.24718v1.pdf",
          "categories": [
            "cs.CV",
            "cs.LG"
          ],
          "canonical_url": "https://arxiv.org/abs/2510.24718",
          "fetch_status": "success",
          "fetch_meta": {
            "http_status": 200,
            "content_type": "text/html; charset=utf-8",
            "length_bytes": 46794,
            "final_url": "https://arxiv.org/abs/2510.24718v1"
          },
          "html_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head> <title>[2510.24718v1] Generative View Stitching</title>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<link href=\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"/static/browse/0.3.4/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#5bbad5\" href=\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n<meta content=\"#ffffff\" name=\"theme-color\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv.css?v=20241206\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/browse_search.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/accordion.js\"></script>\n<script language=\"javascript\" src=\"/static/browse/0.3.4/js/optin-modal.js?v=20250819\"></script>\n<link href=\"https://arxiv.org/abs/2510.24718\" rel=\"canonical\"/>\n<meta content=\"Abstract page for arXiv paper 2510.24718v1: Generative View Stitching\" name=\"description\"/><meta content=\"website\" property=\"og:type\"/>\n<meta content=\"arXiv.org\" property=\"og:site_name\"/>\n<meta content=\"Generative View Stitching\" property=\"og:title\"/>\n<meta content=\"https://arxiv.org/abs/2510.24718v1\" property=\"og:url\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image\"/>\n<meta content=\"/static/browse/0.3.4/images/arxiv-logo-fb.png\" property=\"og:image:secure_url\"/>\n<meta content=\"1200\" property=\"og:image:width\"/>\n<meta content=\"700\" property=\"og:image:height\"/>\n<meta content=\"arXiv logo\" property=\"og:image:alt\"/>\n<meta content=\"Autoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the future. In camera-guided video generation with a predefined camera trajectory, this limitation leads to collisions with the generated scene, after which autoregression quickly collapses. To address this, we propose Generative View Stitching (GVS), which samples the entire sequence in parallel such that the generated scene is faithful to every part of the predefined camera trajectory. Our main contribution is a sampling algorithm that extends prior work on diffusion stitching for robot planning to video generation. While such stitching methods usually require a specially trained model, GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing, a prevalent sequence diffusion framework that we show already provides the affordances necessary for stitching. We then introduce Omni Guidance, a technique that enhances the temporal consistency in stitching by conditioning on both the past and future, and that enables our proposed loop-closing mechanism for delivering long-range coherence. Overall, GVS achieves camera-guided video generation that is stable, collision-free, frame-to-frame consistent, and closes loops for a variety of predefined camera paths, including Oscar Reutersvärd's Impossible Staircase. Results are best viewed as videos at https://andrewsonga.github.io/gvs.\" property=\"og:description\"/>\n<meta content=\"@arxiv\" name=\"twitter:site\"/>\n<meta content=\"summary\" name=\"twitter:card\"/>\n<meta content=\"Generative View Stitching\" name=\"twitter:title\"/>\n<meta content=\"Autoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the...\" name=\"twitter:description\"/>\n<meta content=\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\" name=\"twitter:image\"/>\n<meta content=\"arXiv logo\" name=\"twitter:image:alt\"/>\n<link href=\"/static/browse/0.3.4/css/tooltip.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/> <script src=\"/static/browse/0.3.4/js/mathjaxToggle.min.js\" type=\"text/javascript\"></script> <script src=\"//code.jquery.com/jquery-latest.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\" type=\"text/javascript\"></script>\n<script src=\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/toggle-labs.js?20241022\" type=\"text/javascript\"></script>\n<script src=\"/static/browse/0.3.4/js/cite.js\" type=\"text/javascript\"></script><meta content=\"Generative View Stitching\" name=\"citation_title\"/><meta content=\"Song, Chonghyuk\" name=\"citation_author\"/><meta content=\"Stary, Michal\" name=\"citation_author\"/><meta content=\"Chen, Boyuan\" name=\"citation_author\"/><meta content=\"Kopanas, George\" name=\"citation_author\"/><meta content=\"Sitzmann, Vincent\" name=\"citation_author\"/><meta content=\"2025/10/28\" name=\"citation_date\"/><meta content=\"2025/10/28\" name=\"citation_online_date\"/><meta content=\"https://arxiv.org/pdf/2510.24718\" name=\"citation_pdf_url\"/><meta content=\"2510.24718\" name=\"citation_arxiv_id\"/><meta content=\"Autoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the future. In camera-guided video generation with a predefined camera trajectory, this limitation leads to collisions with the generated scene, after which autoregression quickly collapses. To address this, we propose Generative View Stitching (GVS), which samples the entire sequence in parallel such that the generated scene is faithful to every part of the predefined camera trajectory. Our main contribution is a sampling algorithm that extends prior work on diffusion stitching for robot planning to video generation. While such stitching methods usually require a specially trained model, GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing, a prevalent sequence diffusion framework that we show already provides the affordances necessary for stitching. We then introduce Omni Guidance, a technique that enhances the temporal consistency in stitching by conditioning on both the past and future, and that enables our proposed loop-closing mechanism for delivering long-range coherence. Overall, GVS achieves camera-guided video generation that is stable, collision-free, frame-to-frame consistent, and closes loops for a variety of predefined camera paths, including Oscar Reutersv\\&quot;ard's Impossible Staircase. Results are best viewed as videos at https://andrewsonga.github.io/gvs.\" name=\"citation_abstract\"/>\n</head>\n<body class=\"with-cu-identity\">\n<div class=\"flex-wrap-footer\">\n<header>\n<a class=\"is-sr-only\" href=\"#content\">Skip to main content</a>\n<!-- start desktop header -->\n<div class=\"columns is-vcentered is-hidden-mobile\" id=\"cu-identity\">\n<div class=\"column\" id=\"cu-logo\">\n<a href=\"https://www.cornell.edu/\"><img alt=\"Cornell University\" src=\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\"/></a>\n</div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\"column\" id=\"support-ack\">\n<span id=\"support-ack-url\">We gratefully acknowledge support from the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors.</span>\n<a class=\"btn-header-donate\" href=\"https://info.arxiv.org/about/donate.html\">Donate</a>\n</div>\n</div>\n<div class=\"is-hidden-mobile\" id=\"header\">\n<a aria-hidden=\"true\" href=\"/IgnoreMe\" tabindex=\"-1\"></a>\n<div class=\"header-breadcrumbs is-hidden-mobile\">\n<a href=\"/\"><img alt=\"arxiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\" style=\"height:40px;\"/></a> <span>&gt;</span> <a href=\"/list/cs/recent\">cs</a> <span>&gt;</span> arXiv:2510.24718v1\n  </div>\n<div class=\"columns is-vcentered is-mobile\" style=\"justify-content: flex-end;\">\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option value=\"orcid\">ORCID</option>\n<option value=\"author_id\">arXiv author ID</option>\n<option value=\"help\">Help pages</option>\n<option value=\"full_text\">Full text</option>\n</select>\n</div>\n</div>\n<input name=\"source\" type=\"hidden\" value=\"header\"/>\n<button class=\"button is-small is-cul-darker\">Search</button>\n</div>\n</form>\n</div>\n</div><!-- /end desktop header -->\n<div class=\"mobile-header\">\n<div class=\"columns is-mobile\">\n<div class=\"column logo-arxiv\"><a href=\"https://arxiv.org/\"><img alt=\"arXiv logo\" src=\"/static/browse/0.3.4/images/arxiv-logomark-small-white.svg\" style=\"height:60px;\"/>",
          "raw_text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "text": "BibTeX formatted citation\n×\nloading...\nData provided by:",
          "sections": [],
          "score": 0.5941666666666667,
          "score_breakdown": {
            "relevance": 0.0,
            "recency": 0.9766666666666667,
            "credibility": 0.85,
            "novelty": 0.9
          },
          "headline": "Generative View Stitching Enables Collision-Free Camera-Guided Video Generation",
          "tldr": "Autoregressive video diffusion models struggle with future conditioning, leading to scene collisions in camera-guided generation. Generative View Stitching (GVS) addresses this by sampling entire sequences in parallel, ensuring fidelity to predefined camera trajectories. This novel sampling algorithm, compatible with off-the-shelf models, achieves stable, collision-free, and temporally consistent video generation with long-range coherence.",
          "bullets": [
            "Addresses the inability of autoregressive video diffusion models to guide generation with future conditioning, preventing scene collisions.",
            "Proposes Generative View Stitching (GVS), a parallel sampling algorithm for entire video sequences.",
            "Ensures generated scenes are faithful to every part of a predefined camera trajectory.",
            "Introduces a sampling algorithm extending diffusion stitching for robot planning to video generation.",
            "GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing.",
            "Develops Omni Guidance to enhance temporal consistency and enable a loop-closing mechanism for long-range coherence.",
            "Achieves stable, collision-free, frame-to-frame consistent, and loop-closing camera-guided video generation for various paths."
          ],
          "significance": "This research significantly advances camera-guided video generation by overcoming critical limitations of existing autoregressive models. By enabling stable, collision-free, and temporally consistent video synthesis, GVS opens new possibilities for virtual production, architectural visualization, and creating complex visual narratives with precise camera control. Its compatibility with existing models also facilitates broader adoption and integration into current workflows.",
          "limitations": "The abstract does not explicitly state limitations; however, the computational demands of parallel sampling for extremely long or high-resolution sequences are not discussed. Additionally, the method relies on predefined camera trajectories, not dynamic or interactively controlled ones.",
          "keywords": [
            "Generative View Stitching",
            "Video Generation",
            "Diffusion Models",
            "Camera Guidance",
            "Temporal Consistency",
            "Parallel Sampling",
            "Omni Guidance"
          ],
          "read_time_minutes": 10
        }
      ]
    }
  ],
  "metadata": {
    "generated_at": "2025-10-29T16:48:15.039994",
    "sources_count": 1,
    "papers_count": 3,
    "news_count": 0,
    "total_items": 3
  }
}